# ğŸ§  Bluestaq Local RAG Challenge

This repository contains a local Retrieval-Augmented Generation (RAG) system intended to run without cloud dependencies. It pairs a quantized Llama model with hybrid retrieval (FAISS + BM25) and a CLI for interaction.

---

## ğŸš€ Overview

**Goal:** Build a self-contained RAG pipeline that runs locally and lets you query a document corpus with a small, quantized Llama model.

**Key components:**
- Quantized LLM (GGUF format, Llama 3.2 3B Instruct in `models/gguf`)
- Hybrid retrieval: FAISS (dense) + BM25 (sparse)
- CLI for single queries and chat (`app/app.py`)
- Ingestion pipeline to build corpus and FAISS index (`app/ingest.py`)
- Configurable via `config.yaml`

---

## ğŸ“¦ Repository

**Repository:** https://github.com/ArunMunagala7/bluestaq-local-rag

Project layout (top-level):

- `app/` â€” application code and CLI
- `data/` â€” corpus, index and uploads
- `models/gguf/` â€” GGUF model files
- `config.yaml` â€” runtime configuration
- `requirements.txt` â€” Python dependencies

Key files inside `app/`:

- `app.py` â€” Typer CLI (commands: `query-basic`, `query-rag`, `chat`, `upload`, `bulk-upload`)
- `ingest.py` â€” file processing and FAISS index creation
- `retriever.py` â€” hybrid retrieval logic
- `rag.py` â€” RAG pipeline that combines retrieval + generation

---

## ğŸ§  Model & Config

- Default model path: `models/gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf` (set in `config.yaml` under `model.gguf_path`).
- Make sure you have the GGUF model file present before running queries.
- Adjust runtime parameters in `config.yaml` (context size, threads, GPU layers, retrieval chunking, etc.).

---

## âš™ï¸ Setup

1. Create and activate a Python virtual environment (recommended):

```bash
python -m venv .venv
source .venv/bin/activate
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Verify `config.yaml` points to your model path and desired settings.

---

## â–¶ï¸ Usage (CLI)

Run the Typer CLI via module execution. Example commands:

```bash
# Single-shot LLM only (no retrieval)
python -m app.app query-basic "What is Retrieval Augmented Generation?"

# RAG query (retrieval + generation)
python -m app.app query-rag "Summarize the key ideas from the uploaded paper."

# Interactive chat mode
python -m app.app chat

# Upload a single file into the corpus
python -m app.app upload path/to/file.pdf

# Bulk upload files from data/uploads and rebuild index
python -m app.app bulk-upload
```

Notes:
- If the FAISS index is missing or out of date, run `bulk-upload` to process `data/uploads` and rebuild the index.
- The CLI prints retrieved passages and simple relevance scores when using `query-rag`.

---

## ğŸ“‚ Data

- `data/corpus/` â€” text chunks extracted from source PDFs and documents
- `data/index/` â€” FAISS index files (e.g. `faiss.index`)
- `data/uploads/` â€” drop files here for ingestion

There are sample text files under `data/corpus/` in this repo.

---

## ğŸ§© Notes & Tips

- The project is designed to run locally; ensure you have a compatible CPU/GPU setup and enough RAM for the chosen model and context window.
- For faster model inference, tune `n_threads` and `n_gpu_layers` in `config.yaml`.
- If you prefer running the model via SSH keys/remote or using a different model location, update `config.yaml` accordingly.

---

## ğŸ“š References

- See `app/` for implementation details and `config.yaml` for runtime tuning.

If you want, I can also add a short Quick Start section with exact commands for common setups (CPU-only, macOS with MPS, or using GPU).
# ğŸ§  Bluestaq Local RAG Challenge

This repository contains the complete implementation of a **local Retrieval-Augmented Generation (RAG)** system designed to run entirely on a laptop â€” no cloud dependencies required.  
It integrates a quantized local Llama model with hybrid denseâ€“sparse retrieval and a user-friendly Command-Line Interface (CLI) for natural language interaction.

---

## ğŸš€ Overview

**Goal:**  
To develop a robust, efficient, and fully local language model pipeline that augments generation through document retrieval, while maintaining sub-second response times on consumer hardware.

**Key Components:**
- ğŸ§© Quantized LLM (Llama 3.2 3B Instruct â€” GGUF)
- ğŸ” Hybrid Retrieval (Dense + Sparse using FAISS and BM25)
- ğŸ’¬ CLI-based Query and Chat Interface
- ğŸ—‚ï¸ Local Corpus Management and PDF Ingestion
- âš™ï¸ Configurable Parameters via `config.yaml`

---

## ğŸ“¦ Code Repository

**Repository:** [https://github.com/ArunMunagala7/local-rag](https://github.com/ArunMunagala7/local-rag)

All source code is modularized under `app/`, with separate scripts for:
- **`rag.py`** â†’ retrieval pipeline integration  
- **`retriever.py`** â†’ hybrid dense/sparse search  
- **`ingest.py`** â†’ corpus creation and FAISS index building  
- **`app.py`** â†’ Typer-based CLI entrypoint  
- **`config.yaml`** â†’ centralized configuration and model tuning  

---

## ğŸ§  Model Files

The project uses a **quantized local model**:

