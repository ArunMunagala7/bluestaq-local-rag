# ğŸ§  Bluestaq Local RAG Challenge

This repository contains the complete implementation of a **local Retrieval-Augmented Generation (RAG)** system designed to run entirely on a laptop â€” no cloud dependencies required.  
It integrates a quantized local Llama model with hybrid denseâ€“sparse retrieval and a user-friendly Command-Line Interface (CLI) for natural language interaction.

---

## ğŸš€ Overview

**Goal:**  
To develop a robust, efficient, and fully local language model pipeline that augments generation through document retrieval, while maintaining sub-second response times on consumer hardware.

**Key Components:**
- ğŸ§© Quantized LLM (Llama 3.2 3B Instruct â€” GGUF)
- ğŸ” Hybrid Retrieval (Dense + Sparse using FAISS and BM25)
- ğŸ’¬ CLI-based Query and Chat Interface
- ğŸ—‚ï¸ Local Corpus Management and PDF Ingestion
- âš™ï¸ Configurable Parameters via `config.yaml`

---

## ğŸ“¦ Code Repository

**Repository:** [https://github.com/ArunMunagala7/local-rag](https://github.com/ArunMunagala7/local-rag)

All source code is modularized under `app/`, with separate scripts for:
- **`rag.py`** â†’ retrieval pipeline integration  
- **`retriever.py`** â†’ hybrid dense/sparse search  
- **`ingest.py`** â†’ corpus creation and FAISS index building  
- **`app.py`** â†’ Typer-based CLI entrypoint  
- **`config.yaml`** â†’ centralized configuration and model tuning  

---

## ğŸ§  Model Files

The project uses a **quantized local model**:

