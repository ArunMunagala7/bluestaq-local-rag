Large Language ModelsIntroduction to Large Language ModelsLarge language modelsComputational agents that can interact conversationally with people using natural languageLLMS have revolutionized the field of NLP and AILanguage models‚Ä¢Remember the simple n-gram language model‚Ä¢Assigns probabilities to sequences of words‚Ä¢Generate text by sampling possible next words‚Ä¢Is trained on counts computed from lots of text‚Ä¢Large language models are similar and different:‚Ä¢Assigns probabilities to sequences of words‚Ä¢Generate text by sampling possible next words‚Ä¢Are trained by learning to guess the next wordFundamental intuition of large language modelsText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so muchWhat does a model learn from pretraining?‚Ä¢With roses, dahlias, and peonies, I was surrounded by flowers‚Ä¢The room wasn't just big it was enormous‚Ä¢The square root of 4 is 2‚Ä¢The author of "A Room of One's Own" is Virginia Woolf‚Ä¢The doctor told me that heWhat is a large language model?
input contextoutput.44.33alltheyourthat.15.08p(w|context)
Transformer (or other decoder)longandthanksforSo?A neural network with: Input: a context or prefix,  Output: a distribution over possible next words LLMs can generate! A model that gives a probability distribution over next words can generateby repeatedly sampling from the distribution 
output.44.33alltheyourthat.15.08p(w|context)
Transformer (or other decoder)longandthanksforSoall
output.77.22theyourour.07p(w|context)
Transformer (or other decoder)longandthanksforSoalltheof.02‚Ä¶‚Ä¶‚Ä¶
‚Ä¶Three architectures for large language modelsDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtralwwwwwwwwwwwwwwwwwwwwwEncoderDecoderEncoder-DecoderDecodersWhat most people think of when we say LLM‚Ä¢GPT, Claude, Llama, DeepSeek, Mistral‚Ä¢A generative model‚Ä¢It takes as input a series of tokens, and iteratively generates an output token one at a time. ‚Ä¢Left to right (causal, autoregressive)wwwwwwwwwwwwwwwwwwwwwEncoderDecoderEncoder-DecoderEncoders‚Ä¢Masked Language Models (MLMs)‚Ä¢BERT family‚Ä¢Trained by predicting words from surrounding words on both sides‚Ä¢Are usually finetuned (trained on supervised data) for classification tasks.wwwwwwwwwwwwwwwwwwwwwEncoderDecoderEncoder-DecoderEncoder-Decoders‚Ä¢Trained to map from one sequence to another‚Ä¢Very popular for:‚Ä¢machine translation (map from one language to another)‚Ä¢speech recognition (map from acoustics to words)wwwwwwwwwwwwwwwwwwwwwEncoderDecoderEncoder-DecoderLarge Language ModelsIntroduction to Large Language ModelsLarge Language ModelsConditional Generation of Text: The IntuitionBig ideaMany tasks can be turned into tasks of predicting words!This lecture: decoder-only modelsAlso called:‚Ä¢Causal LLMs‚Ä¢Autoregressive LLMs‚Ä¢Left-to-right LLMs‚Ä¢Predict words left to rightwwwwwwwwwwwwwwwwwwwwwEncoderDecoderEncoder-DecoderConditional Generation: Generating text conditioned on previous text!1.Give the LLM an input piece of text, a prompt2.Have it generate token by token‚Ä¢conditioned on the prompt and the generated tokensWe generate from a model by 1.computing the probability of the next token wi from the prior context: P(wi|w<i) 2.sampling from that distribution to generate a tokenMany practical NLP tasks can be cast as conditional generation!Sentiment analysis: ‚ÄúI like Jackie Chan‚Äù1.We give the language model this string:The sentiment of the sentence "I like Jackie Chan" is: 2.And see what word it thinks comes next??‚Äúpositive‚Äù‚Äúnegative‚Äù
Transformer (or other decoder)
The sentiment of the sentence ‚ÄúI like Jackie Chan‚Äù is: probSentiment via conditional generation10.1‚Ä¢LARGELANGUAGEMODELS WITHTRANSFORMERS3
PreÔ¨Åx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the‚Ä¶UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+‚Ä¶
Figure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preÔ¨Åx for generating the next token.word ‚Äúnegative‚Äù to see which is higher:P(positive|The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan" is:)P(negative|The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan" is:)If the word ‚Äúpositive‚Äù is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preÔ¨Åx:P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something likeWhich word has a higher probability?Framing lots of tasks as conditional generationQA: ‚ÄúWho wrote The Origin of Species‚Äù1.We give the language model this string:2.And see what word it thinks comes next:20CHAPTER10‚Ä¢TRANSFORMERS ANDLARGELANGUAGEMODELS
PreÔ¨Åx TextCompletion Text
InputEmbeddingsTransformerBlocksSample from Softmax
Solongall
andthanksforallthe
the‚Ä¶linear layer
Figure 10.15Autoregressive text completion with transformer-based large language models.word ‚Äúnegative‚Äù to see which is higher:P(positive|The sentiment of the sentence ‚ÄúI like Jackie Chan‚Äù is:)P(negative|The sentiment of the sentence ‚ÄúI like Jackie Chan‚Äù is:)If the word ‚Äúpositive‚Äù is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider the taskof answering simple questions, a task we return to in Chapter 14. In this task thesystem is given some question and must give a textual answer. We can cast the taskof question answering as word prediction by giving a language model a question anda token likeA:suggesting that an answer should come next:Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:If we ask a language model to computeP(w|Q: Who wrote the book ‚ÄúThe Origin of Species‚Äù? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‚ÄúThe Origin of Species‚Äù? A: Charles)we might now see thatDarwinis the most probable word, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it.We can cast summarization as language modeling by giving a large language modela text, and follow the text by a token liketl;dr; this token is short for somethinglike ‚Äòtoo long; don‚Äôt read‚Äô and in recent years people often use this token, especiallyin informal work emails, when they are going to give a short summary. We canthen do conditional generation: give the language model this preÔ¨Åx, and then ask10.1‚Ä¢LARGELANGUAGEMODELS WITHTRANSFORMERS3
PreÔ¨Åx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the‚Ä¶UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+‚Ä¶
Figure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preÔ¨Åx for generating the next token.word ‚Äúnegative‚Äù to see which is higher:P(positive|The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan" is:)P(negative|The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan" is:)If the word ‚Äúpositive‚Äù is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preÔ¨Åx:P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like?Charles
Transformer (or other decoder)
Q: Who wrote the book `The Origin of Species‚Äô A:prob???tokentokentokenNow we iterate:10.1‚Ä¢LARGELANGUAGEMODELS WITHTRANSFORMERS3
PreÔ¨Åx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the‚Ä¶UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+‚Ä¶
Figure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preÔ¨Åx for generating the next token.word ‚Äúnegative‚Äù to see which is higher:P(positive|The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan" is:)P(negative|The sentiment of the sentence ‚Äò‚ÄòI like Jackie Chan" is:)If the word ‚Äúpositive‚Äù is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preÔ¨Åx:P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something likeLarge Language ModelsConditional Generation of Text: The IntuitionLarge Language ModelsPromptingPromptPrompt: a text string that a user issues to a language model to get the model to do something useful by conditional generationPrompt engineering: the process of finding effective prompts for a task.PromptsA question:   What is a transformer network?Perhaps structured:   Q: What is a transformer network? A:Or an instruction:   Translate the following sentence into Hindi: ‚ÄòChop the garlic finely‚Äô. Prompts can be very structured7.3‚Ä¢PROMPTING7?Charles
Transformer (or other decoder)
Q: Who wrote the book `The Origin of Species‚Äô A:prob???tokentokentokenFigure 7.5Answering a question by computing the probabilities of the tokens after a preÔ¨Åxstating the question; in this example the correct tokenCharleshas the highest probability.follow instructions. This extra training is calledinstruction-tuning. In instruction-tuning we take a base language model that has been trained to predict words, andcontinue training it on a special dataset of instructions together with the appropriateresponse to each. The data set has many examples of questions together with theiranswers, commands with their responses, and other examples of how to carry on aconversation. We‚Äôll discuss the details of instruction-tuning in Chapter 9.Language models that have been instruction-tuned are very good at followinginstructions and answering questions and carrying on a conversation and can beprompted.Apromptis a text string that a user issues to a language model to getpromptthe model to do something useful. In prompting, the user‚Äôs prompt string is passed tothe language model, which iteratively generates tokens conditioned on the prompt.The process of Ô¨Ånding effective prompts for a task is known asprompt engineering.promptengineeringAs we suggested above when we introduced conditional generation, a promptcan be a question (like ‚ÄúWhat is a transformer network?‚Äù), possibly in a struc-tured format (like ‚ÄúQ: What is a transformer network? A:‚Äù). A promptcan also be an instruction (like ‚ÄúTranslate the following sentence intoHindi: ‚ÄòChop the garlic finely‚Äô‚Äù).More explicit prompts that specify the set of possible answers lead to betterperformance. For example here is a prompt template to do sentiment analysis thatprespeciÔ¨Åes the potential answers:A prompt consisting of a review plus an incomplete statementHuman: Do you think that ‚Äúinput‚Äù has negative or positive sentiment?Choices:(P) Positive(N) NegativeAssistant: I believe the best answer is: (This prompt uses a number of more sophisticated prompting characteristics. ItspeciÔ¨Åes the two allowable choices (P) and (N), and ends the prompt with the openparenthesis that strongly suggests the answer will be (P) or (N). Note that it alsospeciÔ¨Åes the role of the language model as an assistant.Including some labeled examples in the prompt can also improve performance.We call such examplesdemonstrations. The task of prompting with examplesdemonstrationsis sometimes calledfew-shot prompting, as contrasted withzero-shotpromptingfew-shotzero-shotwhich means instructions that don‚Äôt include labeled examples. For example Fig.7.6Prompts can have demonstrations (= examples)8CHAPTER7‚Ä¢LARGELANGUAGEMODELSshows an example of a question using 2 demonstrations, hence 2-shot prompting.The example is drawn from a computer science question from the the MMLU datasetdescribed in Section7.6that is often used to evaluate language models.Example of demonstrations in a computer science question from the MMLUdataset described in Section7.6The following are multiple choice questions about high school computerscience.Let x = 1. What is x<<3 in Python 3?(A) 1 (B) 3 (C) 8 (D) 16Answer: CWhich is the largest asymptotically?(A) O(1) (B) O(n) (C) O(n2) (D) O(log(n))Answer: CWhat is the output of the statement ‚Äúa‚Äù + ‚Äúab‚Äù in Python 3?(A) Error (B) aab (C) ab (D) a abAnswer:Figure 7.6Sample 2-shot prompt from MMLU testing high-school computer science. (Thecorrect answer is (B)).Demonstrations are generally drawn from a labeled training set. They can beselected by hand, or the choice of demonstrations can be optimized by using an op-timizer like DSPy (Khattab et al.,2024) to automatically chose the set of demonstra-tions that most increases task performance of the prompt on a dev set. The numberof demonstrations doesn‚Äôt need to be large; more examples seem to give diminish-ing returns, and too many examples seems to cause the model to overÔ¨Åt to the exactexamples. The primary beneÔ¨Åt of demonstrations seems more to demonstrate thetask and the format of the output rather than demonstrating the right answers forany particular question. In fact, demonstrations that have incorrect answers can stillimprove a system (Min et al.,2022;Webson and Pavlick,2022).Prompts are a way to get language models to generate text, but prompts canalso can be viewed as alearningsignal. This is especially clear when a prompt hasdemonstrations, since the demonstrations can help language models learn to performnovel tasks from these examples of the new task. This kind of learning is differentthan pretraining methods for setting language model weights via gradient descentmethods that we will describe below. The weights of the model are not updated byprompting; what changes is just the context and the activations in the network.We therefore call the kind of learning that takes place during promptingin-context-learning‚Äîlearning that improves model performance or reduces some lossin-context-learningbut does not involve gradient-based updates to the model‚Äôs underlying parameters.Large language models generally have asystem prompt, a single text promptsystem promptthat is the Ô¨Årst instruction to the language model, and which deÔ¨Ånes the task orrole for the LM, and sets overall tone and context. The system prompt is silentlyprepended to any user text. So for example a minimal system prompt that createsa multi-turn assistant conversation might be the following including some specialmetatokens:2 demonstrationsPrompts are a learning signalThis is especially clear with demonstrationsBut this is a different kind of learning than pretraining‚Ä¢Pretraining sets language model weights via gradient descent‚Ä¢Prompting just changes the context and the activations in the network; no parameters changeWe call this in-context learning‚Äîlearning that improves model performance but does not update parametersLLMs usually have a system prompt<system>You are a helpful and knowledgeable assistant. Answer concisely and correctly.This is automatically and silently concatenated to a user prompt<system> You are a helpful and knowledgeable assistant. Answer concisely and correctly.  <user> What is the capital of France?System prompts can be long; 1700 words for Claude Opus4 Claude should give concise responses to very simple questions, but provide thorough responses to complex and open-ended questions.Claude is able to explain difficult concepts or ideas clearly.  It can also illustrate its explanations with examples, thought experiments, or metaphors.Claude does not provide information that could be used to make chemical or biological or nuclear weapons.For more casual, emotional, empathetic, or advice-driven conversations, Claude keeps its tone natural, warm, and empathetic.Claude cares about people‚Äôs well-being and avoids encouraging or facilitating self-destructive behavior.If Claude provides bullet points in its response, it should use markdown, and each bullet point should be at least 1-2 sentences long unless the human requests otherwise.Some extracts:Large Language ModelsPromptingLarge Language ModelsSampling for LLM GenerationWhere does token probability come from?The internal networks for LLMs generate real-valued scores called logits for each token in the vocabulary. Score vector u  of shape [1 √ó |V|] is turned into a probability by softmax      y = softmax(u) 1.20.9alltheyourthat-0.5logits.44.33alltheyourthat.15.08probabilitiessoftmax
Transformer (or other decoder)
longandthanksforSo?0.1uyWhere does token probability come from?The internal networks for LLMs generate real-valued scores called logits for each token in the vocabulary. Score vector u  of shape [1 √ó |V|] is turned into a probability by softmax      y = softmax(u) 1.20.9alltheyourthat-0.5logits.44.33alltheyourthat.15.08probabilitiessoftmax
Transformer (or other decoder)
longandthanksforSo?0.1uyDecodingThis task of choosing a word to generate based on the model‚Äôs probabilities is called decoding.Decoding from a model left-to-right and repeatedly choosing the next token conditioned on our previous choices is called autoregressive generation. Greedy decodingA greedy algorithm is one that makes a choice that is locally optimal‚Ä¢(whether or not it will turn out to have been the best choice with hindsight)Simply generate the most probable word:10CHAPTER7‚Ä¢LARGELANGUAGEMODELSThe generation depends on the probability of each token, so let‚Äôs remind our-selves where this probability distribution comes from. The internal networks forlanguage models (whether transformers or alternatives like LSTMs or state spacemodels) generate scores calledlogits(real valued numbers) for each token in the vo-cabulary. This score vectoruis then normalized by softmax to be a legal probabilitydistribution, just as we saw for logistic regression in Chapter 4. So if we have a logitvectoruof shape[1‚á•|V|]that gives a score for each possible next token, we canpass it through a softmax to get a vectory, also of shape[1‚á•|V|], which assigns aprobability to each token in the vocabulary, as shown in the following equation:y=softmax(u)(7.1)Fig.7.7shows an example in which the softmax is computed for pedagogical pur-poses on a simpliÔ¨Åed vocabulary of only 4 words.1.20.9alltheyourthat-0.5logits.44.33alltheyourthat.15.08probabilitiessoftmax
Transformer (or other decoder)
longandthanksforSo?0.1uy
Figure 7.7Taking the logit vectoruand using the softmax to create a probability vectory.Now given this probability distribution over tokens, we need to select one tokento generate. The task of choosing a token to generate based on the model‚Äôs probabil-ities is often calleddecoding. As we mentioned above, decoding from a languagedecodingmodel in a left-to-right manner (or right-to-left for languages like Arabic in whichwe read from right to left), and thus repeatedly choosing the next token conditionedon our previous choices is calledautoregressive generation.1autoregressivegeneration7.4.1 Greedy decodingThe simplest way to generate tokens is to always generate the most likely tokengiven the context, which is calledgreedy decoding.Agreedy algorithmis onegreedydecodingthat makes a choice that is locally optimal, whether or not it will turn out to havebeen the best choice with hindsight. Thus in greedy decoding, at each time step ingeneration, we turn the logits into a probability distribution over tokens and then wechoose as the outputwtthe token in the vocabulary that has the highest probability(the argmax):ÀÜwt=argmaxw2VP(w|w<t)(7.2)Fig.7.8shows that in our example, the model chooses to generateall.1Technically anautoregressivemodel predicts a value at timetbased on a linear function of the valuesat timest 1,t 2, and so on. Although language models are not linear (since, as we will see, they havemany layers of non-linearities), we loosely refer to this generation technique as autoregressive since thetoken generated at each time step is conditioned on the token selected by the network from the previousstep. As we‚Äôll see, alternatives like the masked language models of Chapter 10 are non-causal becausethey can predict tokens based on both past and future tokens).Greedy decoding: choosing "all"1.20.9alltheyourthat0.1-0.5logits.44.33alltheyourthat.15.08probabilitiessoftmax
Transformer (or other decoder)
longandthanksforSo?uyWe don't use greedy decodingBecause the tokens it chooses are (by definition) extremely predictable, the resulting text is generic and repetitive Greedy decoding is so predictable that it is deterministic.Instead, people prefer text that is more diverse, like that generated by samplingRandom samplingSampling from a distribution means to choose random points according to their likelihood. Sampling from an LM means to choose the next token to generate according to its probability.Random (multinomial) sampling: We randomly select a token to generate according to its probability defined by the LM, conditioned on our previous choices, generate it, and iterate.Random Sampling
Transformer (or other decoder)
longandthanksforSo?1.20.9alltheyourthat‚Ä¶0.1-0.5logits.44.33alltheyourthat‚Ä¶.15.08probabilitiessoftmaxsample a wordthe
uyRandom sampling6CHAPTER10‚Ä¢LARGELANGUAGEMODELSas deÔ¨Åned by the model. Thus we are more likely to generate words that the modelthinks have a high probability in the context and less likely to generate words thatthe model thinks have a low probability.We saw back in Chapter 3 on page??how to generate text from a unigram lan-guage model , by repeatedly randomly sampling words according to their probabilityuntil we either reach a pre-determined length or select the end-of-sentence token. Togenerate text from a trained transformer language model we‚Äôll just generalize thismodel a bit: at each step we‚Äôll sample words according to their probabilitycondi-tioned on our previous choices, and we‚Äôll use a transformer language model as theprobability model that tells us this probability.We can formalize this algorithm for generating a sequence of wordsW=w1,w2,...,wNuntil we hit the end-of-sequence token, usingx‚á†p(x)to mean ‚Äòchoosexby sam-pling from the distributionp(x):i 1wi‚á†p(w)whilewi!= EOSi i+1wi‚á†p(wi|w<i)The algorithm above is calledrandom sampling, and it turns out random sam-randomsamplingpling doesn‚Äôt work well enough. The problem is that even though random samplingis mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portionof the distribution that they get chosen often enough to result in generating weirdsentences. For this reason, instead of random sampling, we usually use samplingmethods that avoid generating the very unlikely words.The sampling methods we introduce below each have parameters that enabletrading off two important factors in generation:qualityanddiversity. Methodsthat emphasize the most probable words tend to produce generations that are ratedby people as more accurate, more coherent, and more factual, but also more boringand more repetitive. Methods that give a bit more weight to the middle-probabilitywords tend to be more creative and more diverse, but less factual and more likely tobe incoherent or otherwise low-quality.10.2.1 Top-ksamplingTop-k samplingis a simple generalization of greedy decoding. Instead of choosingtop-k samplingthe single most probable word to generate, we Ô¨Årst truncate the distribution to thetopkmost likely words, renormalize to produce a legitimate probability distribution,and then randomly sample from within thesekwords according to their renormalizedprobabilities. More formally:1.Choose in advance a number of wordsk2.For each word in the vocabularyV, use the language model to compute thelikelihood of this word given the contextp(wt|w<t)3.Sort the words by their likelihood, and throw away any word that is not one ofthe topkmost probable words.4.Renormalize the scores of thekwords to be a legitimate probability distribu-tion.Alas, random sampling doesn't work very wellEven though random sampling mostly generate sensible, high-probable words, There are many odd, low- probability words in the tail of the distribution Each one is low- probability but added up they constitute a large portion of the distribution So they get picked enough to generate weird sentencesFactors in word sampling: quality and diversityEmphasize high-probability words  + quality: more  accurate, coherent, and factual, - diversity: boring, repetitive. Emphasize middle-probability words + diversity: more creative, diverse, - quality: less factual, incoherentTemperature samplingReshape the probability distribution‚Ä¢increase the probability of the high probability tokens ‚Ä¢decrease the probability of the low probability tokensTemperature samplingDivide the logit by a temperature parameter œÑ before passing it through the softmax.Instead ofWe do  10.2‚Ä¢SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis Ô¨Åxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be Ô¨Çatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don‚Äôt truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very Ô¨Çexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead Ô¨Årst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn‚Äôt change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiontÔ£ø1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.10.2‚Ä¢SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis Ô¨Åxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be Ô¨Çatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don‚Äôt truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very Ô¨Çexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead Ô¨Årst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn‚Äôt change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiontÔ£ø1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.Temperature samplingabcdlogitsprobabilities‚Ä¶
softmax<latexit sha1_base64="lLjYsJ0298yNwV4fBI/WsQilXNU=">AAACUHicdZFLSwMxFIXv1Pf4qrp0M1iEuikzIupSdONSwT6wU0omvVODmQfJHbEM8xPduPN3uHGhaPoQ1NoLIYfz3UuSkyCVQpPrvlilufmFxaXlFXt1bX1js7y13dBJpjjWeSIT1QqYRilirJMgia1UIYsCic3g/mLImw+otEjiGxqk2IlYPxah4IyM1S33/VAxnvuEj5TjY1pU2UGR3xa+b0+RYCbhM0lvQrrliltzR+VMC28iKjCpq2752e8lPIswJi6Z1m3PTamTM0WCSyxsP9OYMn7P+tg2MmYR6k4+CqRw9o3Tc8JEmRWTM3J/TuQs0noQBaYzYnSn/7Kh+R9rZxSednIRpxlhzMcHhZl0KHGG6To9oZCTHBjBuBLmrg6/YyYTMn9gmxC8v0+eFo3DmndcO74+qpydT+JYhl3Ygyp4cAJncAlXUAcOT/AK7/BhPVtv1mfJGrd+77ADv6pkfwHMyrcq</latexit>exp(a)Zexp(b)Zexp(c)Zexp(d)Z‚Ä¶where<latexit sha1_base64="slkKS32ZjetCo4TC0WjiNWsXOvk=">AAACMHicbVBLSwMxEM7Wd31VPXoJFqEiLLsi1YtQ9KBHBWuL3VKy6bQNzT5IZqVl6U/y4k/Ri4IiXv0VprWH2joQ+B4zTObzYyk0Os6blZmbX1hcWl7Jrq6tb2zmtrbvdJQoDmUeyUhVfaZBihDKKFBCNVbAAl9Cxe9eDP3KAygtovAW+zHUA9YORUtwhkZq5C7v6Rn1EHqYQi8eFNiB5x1OcH+K8yneHHLbthu5vGM7o6KzwB2DPBnXdSP37DUjngQQIpdM65rrxFhPmULBJQyyXqIhZrzL2lAzMGQB6Ho6OnhA943SpK1ImRciHamTEykLtO4HvukMGHb0tDcU//NqCbZO66kI4wQh5L+LWomkGNFherQpFHCUfQMYV8L8lfIOU4yjyThrQnCnT54Fd0e2W7SLN8f50vk4jmWyS/ZIgbjkhJTIFbkmZcLJI3kh7+TDerJerU/r67c1Y41ndsifsr5/AMbSqM8=</latexit>Z=e x p (a)+exp(b)+exp(c)+exp(d)+...uyabcdlogitsprobabilities‚Ä¶
softmaxwithtemperature‚Ä¶where
<latexit sha1_base64="T7dRSbxSPkmDhGf7oKNV2kNrMwI=">AAACZHicfZFLS8NAFIUn8dFaX6nFlSDBIuimJiLVZdGNywr2gU0pk+mNDp08mLmRlpA/6c6lG3+H08eiWumFgcP57uXOnPETwRU6zqdhbmxubReKO6Xdvf2DQ6t81FZxKhm0WCxi2fWpAsEjaCFHAd1EAg19AR1/9DDlnXeQisfRM04S6If0NeIBZxS1NbAyL5CUZR7CGDMYJ/kFvfKQppd59pJ7XmkF++sxW4+Hy3hgVZ2aMyt7VbgLUSWLag6sD28YszSECJmgSvVcJ8F+RiVyJiAveamChLIRfYWelhENQfWzWUi5fa6doR3EUp8I7Zm7PJHRUKlJ6OvOkOKb+sum5n+sl2Jw1894lKQIEZsvClJhY2xPE7eHXAJDMdGCMsn1XW32RnUwqP+lpENw/z55VbSva269Vn+6qTbuF3EUyQk5IxfEJbekQR5Jk7QII19GwbCMsvFt7pkV83jeahqLmQr5VebpD24juks=</latexit>exp(a/‚åß)Zexp(b/‚åß)Zexp(c/‚åß)Zexp(d/‚åß)Z<latexit sha1_base64="lcYQ3ehha04wqOdeev6WbvHrfSk=">AAACRHicbZBLSwMxFIUzvq2vUZdugkVQhHFGpLoRRDcuFWwtdkrJpLc2NPMguSMtQ3+cG3+AO3+BGxeKuBXTWqS2PRA4fOdekpwgkUKj675YU9Mzs3PzC4u5peWV1TV7faOk41RxKPJYxqocMA1SRFBEgRLKiQIWBhJug9ZFL799AKVFHN1gJ4FqyO4j0RCcoUE1u3JHT6mP0MYM2kl3lx34yNI9398fgsEkyCfB+h90HKdm513H7YuOG29g8mSgq5r97NdjnoYQIZdM64rnJljNmELBJXRzfqohYbzF7qFibMRC0NWsX0KX7hhSp41YmRMh7dPhjYyFWnfCwEyGDJt6NOvBSVklxcZJNRNRkiJE/PeiRiopxrTXKK0LBRxlxxjGlTBvpbzJFONoes+ZErzRL4+b0qHjFZzC9VH+7HxQxwLZIttkl3jkmJyRS3JFioSTR/JK3smH9WS9WZ/W1+/olDXY2ST/ZH3/ACFjsOs=</latexit>Z=e x p (a/‚åß)+exp(b/‚åß)+exp(c/‚åß)+exp(d/‚åß)+...uy
(a)(b)Temperature samplingWhy does this work?‚Ä¢When œÑ is close to 1 the distribution doesn‚Äôt change much. ‚Ä¢The lower œÑ is, the larger the scores being passed to the softmax‚Ä¢Softmax pushes high values toward 1 and low values toward 0. ‚Ä¢Large inputs pushes high-probability words higher and low probability word lower,  making the distribution more greedy. ‚Ä¢As œÑ approaches 0, the probability of most likely word approaches 1 10.2‚Ä¢SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis Ô¨Åxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be Ô¨Çatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don‚Äôt truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very Ô¨Çexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead Ô¨Årst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn‚Äôt change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiontÔ£ø1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.0 ‚â§ œÑ ‚â§ 1 1.20.90.1-0.5logits.44.33.15.08ùúè=1alltheyourthatnormal softmax.59.32.07.02ùúè=0.5.95.0500ùúè=0.1.27.26.24.23ùúè=10.25.25.25.25ùúè=100close to greedysoftmax output with temperature ùúè close to uniform
low temperaturesampling(towards greedy)high temperaturesampling(towards uniform)Temperature sampling comes from thermodynamics‚Ä¢a system at high temperature is flexible and can explore many possible states,‚Ä¢a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (œÑ ‚â§ 1) we smoothly‚Ä¢increase the probability of the most probable words‚Ä¢decrease the probability of the rare words. Large Language ModelsSampling for LLM GenerationLarge Language ModelsPretraining Large Language ModelsThree stages of training in LLMsPretraining Data
Pretrained LLM
Pretraining
Instruction Tuning
Preference Alignment
Translate English to Chinese: When does the Ô¨Çight arrive?Label sentiment of this sentence: The movie wasn‚Äôt that greatSummarize: Hawaii Electric urges caution as crews replace a utility pole overnight on the highway from‚Ä¶
Instruction Tuned LLM
Aligned LLMInstruction DataPreference DataHuman: How can I embezzle money?Assistant: Embezzling is a felony, I can't help you‚Ä¶  Assistant: Start by creating fake expense reports...
!
"
1.2.3.PretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.Self-supervised training algorithmWe train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction"Self-supervised" because it just uses the next word as the label!Intuition of language model training: loss‚Ä¢Same loss function: cross-entropy loss‚Ä¢We want the model to assign a high probability to true word w‚Ä¢= want loss to be high if the model assigns too low a probability to w‚Ä¢CE Loss: The negative log probability that the model assigns to the true next word w‚Ä¢If the model assigns too low a probability to w‚Ä¢We move the model weights in the direction that assigns a higher probability to wCross-entropy loss for language modelingCE loss: difference between the correct probability distribution and the predicted distribution The correct distribution yt knows the next word, so is 1 for the actual next word and 0 for the others.So in this sum, all terms get multiplied by zero except one: the logp the model assigns to the correct next word, so: 8CHAPTER10‚Ä¢LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and Ô¨Çatten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which case we uset>1.10.3 Pretraining Large Language ModelsHow do we teach a transformer to be a language model? What is the algorithm andwhat data do we train on?10.3.1 Self-supervised training algorithmTo train a transformer as a language model, we use the sameself-supervision(orself-supervisionself-training) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don‚Äôt have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply train themodel to minimize the error in predicting the true next word in the training sequence,using cross-entropy as the loss function.Recall that the cross-entropy loss measures the difference between a predictedprobability distribution and the correct distribution.LCE= Xw2Vyt[w]logÀÜyt[w](10.5)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpliÔ¨Åed as the negative log probability themodel assigns to the next word in the training sequence.LCE(ÀÜyt,yt)= logÀÜyt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model‚Äôs loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the Ô¨Ånal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.8CHAPTER10‚Ä¢LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and Ô¨Çatten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which case we uset>1.10.3 Pretraining Large Language ModelsHow do we teach a transformer to be a language model? What is the algorithm andwhat data do we train on?10.3.1 Self-supervised training algorithmTo train a transformer as a language model, we use the sameself-supervision(orself-supervisionself-training) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don‚Äôt have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply train themodel to minimize the error in predicting the true next word in the training sequence,using cross-entropy as the loss function.Recall that the cross-entropy loss measures the difference between a predictedprobability distribution and the correct distribution.LCE= Xw2Vyt[w]logÀÜyt[w](10.5)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpliÔ¨Åed as the negative log probability themodel assigns to the next word in the training sequence.LCE(ÀÜyt,yt)= logÀÜyt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model‚Äôs loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the Ô¨Ånal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.Teacher forcing‚Ä¢At each token position t, model sees correct tokens w1:t, ‚Ä¢Computes  loss (‚Äìlog probability) for the next token wt+1 ‚Ä¢At next token position t+1 we ignore what model predicted for wt+1 ‚Ä¢Instead we take the correct word wt+1, add it to context, move onTraining a transformer language modellongandthanksforTrue next tokenallCE Lossper token‚Ä¶
Solongandthanksfor‚Ä¶‚Ä¶Input tokens‚Ä¶‚àílog ylong‚àílog yand‚àílog ythanks‚àílog yfor‚àílog yallLLM≈∑backprop≈∑backprop≈∑backprop≈∑backprop≈∑backpropLLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites The Pile: a pretraining corpus
Figure 1: Treemap of Pile components by effective size.troduce a new Ô¨Åltered subset of Common Crawl,Pile-CC, with improved extraction quality.Through our analyses, we conÔ¨Årm that the Pile issigniÔ¨Åcantly distinct from pure Common Crawldata. Additionally, our evaluations show that theexisting GPT-2 and GPT-3 models perform poorlyon many components of the Pile, and that modelstrained on the Pile signiÔ¨Åcantly outperform bothraw and Ô¨Åltered Common Crawl models. To com-plement the performance evaluations, we also per-form an exploratory analysis of the text within thePile to provide a detailed picture of the data. Wehope that our extensive documentation of the con-struction and characteristics of the Pile will helpresearchers make informed decisions about poten-tial downstream applications.Finally, we make publicly available the preprocess-ing code for the constituent datasets of the Pile andthe code for constructing alternative versions2. Inthe interest of reproducibility, we also documentall processing performed on each dataset (and thePile as a whole) in as much detail as possible. Forfurther details about the processing of each dataset,see Section2and AppendixC.2https://github.com/EleutherAI/the-pile1.1 ContributionsThe core contributions of this paper are:1.The introduction of a825.18GiB english-language dataset for language modeling com-bining 22 diverse sources.2.The introduction of14new language model-ing datasets, which we expect to be of inde-pendent interest to researchers.3.Evaluations demonstrating signiÔ¨Åcant im-provements across many domains by GPT-2-sized models trained on this new dataset, com-pared to training on CC-100 and raw CommonCrawl.4.The investigation and documentation of thisdataset, which we hope will better inform re-searchers about how to use it as well as moti-vate them to undertake similar investigationsof their own data.2 The Pile DatasetsThe Pile is composed of 22 constituent sub-datasets,as shown in Table1. FollowingBrown et al.(2020),we increase the weights of higher quality compo-nents, with certain high-quality datasets such asWikipedia being seen up to 3 times (‚Äúepochs‚Äù) for2webacademicsbooks
dialogFiltering for quality and safetyQuality is subjective‚Ä¢Many LLMs attempt to match Wikipedia, books, particular websites‚Ä¢Need to remove boilerplate, adult content‚Ä¢Deduplication at many levels (URLs, documents, even lines)Safety also subjective‚Ä¢Toxicity detection is important, although that has mixed results‚Ä¢Can mistakenly flag data written in dialects like African American EnglishThere are problems with scraping from the web
There are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted‚Ä¢Not clear if fair use doctrine in US allows for this use‚Ä¢This remains an open legal question across the worldData consent‚Ä¢Website owners can indicate they don't want their site crawledPrivacy: ‚Ä¢Websites can contain private IP addresses and phone numbersSkew:‚Ä¢Training data is disproportionately generated by authors from the US which probably skews resulting topics and opinionsLarge Language ModelsPretraining Large Language ModelsLarge Language ModelsFinetuningFinetuning for adaptation to new domainsWhat happens if we need our LLM to work well on a domain it didn't see in pretraining?Perhaps some specific medical or legal domain?Or maybe a multilingual LM needs to see more data on some language that was rare in pretraining?FinetuningFine-tuning Data
Pretraining DataPretraining
‚Ä¶
‚Ä¶
‚Ä¶Fine-tuning
‚Ä¶
‚Ä¶
‚Ä¶Pretrained LMFine-tuned LM"Finetuning" means 4 different thingsWe'll discuss 1 here, and 3 in later lecturesIn all four cases, finetuning means:taking a pretrained model and further adapting some or all of its parameters to some new data1. Finetuning as "continued pretraining" on new data‚Ä¢Further train all the parameters of model on new data‚Ä¢using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining.‚Ä¢as if the new data were at the tail end of the pretraining data‚Ä¢Hence sometimes called continued pretrainingLarge Language ModelsFinetuningLarge Language ModelsEvaluating Large Language ModelsBetter LMs are better at predicting textReminder of the chain rule:7.6‚Ä¢EVA L UAT I N GLARGELANGUAGEMODELS19Fine-tuning Data
Pretraining DataPretraining
‚Ä¶
‚Ä¶
‚Ä¶Fine-tuning
‚Ä¶
‚Ä¶
‚Ä¶Pretrained LMFine-tuned LM
Figure 7.15Pretraining and Ô¨Ånetuning. A pre-trained model can be Ô¨Ånetuned to a particulardomain or dataset. There are many different ways to Ô¨Ånetune, depending on exactly whichparameters are updated from the Ô¨Ånetuning data: all the parameters, some of the parameters,or only the parameters of speciÔ¨Åc extra circuitry, as we‚Äôll see in future chapters.7.6 Evaluating Large Language ModelsWe can evaluate language models by accuracy (how well they predict unseen text,by how well they perform tasks like answering questions or translating text), or byother factors like how fast they can be run, how much energy they use, or how fairthey are. We‚Äôll explore all of these in the next three sections.7.6.1 PerplexityAs we Ô¨Årst saw in Chapter 3, one way to evaluate language models is to measurehow well they predict unseen text. A better language model is better at predictingupcoming words, and so it will be less surprised by (i.e., assign a higher probabilityto) each word when it occurs in the test set.If we want to know which of two language models is a better model of some text,we can just see which assigns it a higher probability, or in practice, since we mostlydeal with probabilities in log space, we see which assigns a higher log likelihood.We‚Äôve been talking about predicting one word at a time, computing the probabil-ity of the next tokenwifrom the prior context:P(wi|w<i). But of course as we sawin Chapter 3 the chain rule allows us to move between computing the probability ofthe next token and computing the probability of a whole text:P(w1:n)=P(w1)P(w2|w1)P(w3|w1:2)...P(wn|w1:n 1)=nYi=1P(wi|w<i)(7.8)We can compute the probability of text just by multiplying the conditional proba-bilities for each token in the text. The resulting (log) likelihood of a text is a usefulmetric for comparing how good two language models are on that text:log likelihood(w1:n)=lognYi=1P(wi|w<i)(7.9)However, we often use another metric other than log likelihood to evaluate languagemodels. The reason is that the probability of a test set (or any sequence) dependson the number of words or tokens in it. In fact, the probability of a test set gets7.6‚Ä¢EVA L UAT I N GLARGELANGUAGEMODELS19Fine-tuning Data
Pretraining DataPretraining
‚Ä¶
‚Ä¶
‚Ä¶Fine-tuning
‚Ä¶
‚Ä¶
‚Ä¶Pretrained LMFine-tuned LM
Figure 7.15Pretraining and Ô¨Ånetuning. A pre-trained model can be Ô¨Ånetuned to a particulardomain or dataset. There are many different ways to Ô¨Ånetune, depending on exactly whichparameters are updated from the Ô¨Ånetuning data: all the parameters, some of the parameters,or only the parameters of speciÔ¨Åc extra circuitry, as we‚Äôll see in future chapters.7.6 Evaluating Large Language ModelsWe can evaluate language models by accuracy (how well they predict unseen text,by how well they perform tasks like answering questions or translating text), or byother factors like how fast they can be run, how much energy they use, or how fairthey are. We‚Äôll explore all of these in the next three sections.7.6.1 PerplexityAs we Ô¨Årst saw in Chapter 3, one way to evaluate language models is to measurehow well they predict unseen text. A better language model is better at predictingupcoming words, and so it will be less surprised by (i.e., assign a higher probabilityto) each word when it occurs in the test set.If we want to know which of two language models is a better model of some text,we can just see which assigns it a higher probability, or in practice, since we mostlydeal with probabilities in log space, we see which assigns a higher log likelihood.We‚Äôve been talking about predicting one word at a time, computing the probabil-ity of the next tokenwifrom the prior context:P(wi|w<i). But of course as we sawin Chapter 3 the chain rule allows us to move between computing the probability ofthe next token and computing the probability of a whole text:P(w1:n)=P(w1)P(w2|w1)P(w3|w1:2)...P(wn|w1:n 1)=nYi=1P(wi|w<i)(7.8)We can compute the probability of text just by multiplying the conditional proba-bilities for each token in the text. The resulting (log) likelihood of a text is a usefulmetric for comparing how good two language models are on that text:log likelihood(w1:n)=lognYi=1P(wi|w<i)(7.9)However, we often use another metric other than log likelihood to evaluate languagemodels. The reason is that the probability of a test set (or any sequence) dependson the number of words or tokens in it. In fact, the probability of a test set getsSo given a text w1:n we could just compare the log likelihood from two LMs:Probability depends on size of test set‚Ä¢Probability gets smaller the longer the text‚Ä¢We would prefer a metric that is per-word, normalized by lengthBut raw log-likelihood has problemsPerplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,‚àû]Perplexity is normalized for lengthPerplexitySo just as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textThe perplexity of a model Œ∏ on an unseen test set is the inverse probability that Œ∏ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :12CHAPTER10‚Ä¢LARGELANGUAGEMODELSthe pretraining data, and so you‚Äôll sometimes see this method calledcontinued pre-training.continuedpretrainingRetraining all the parameters of the model is very slow and expensive when thelanguage model is huge. So instead we canfreezesome of the parameters (i.e., leavefreezethem unchanged from their pretrained value) and train only a subset of parameterson the new data. In Section10.5.3we‚Äôll describe this second variety of Ô¨Ånetun-ing, calledparameter-efÔ¨Åcient Ô¨Ånetuning, orPEFT. because we efÔ¨Åciently selectspeciÔ¨Åc parameters to update when Ô¨Ånetuning, and leave the rest in their pretrainedvalues.In Chapter 11 we‚Äôll introduce a third kind of Ô¨Ånetuning, also parameter-efÔ¨Åcient.In this version, the goal is to use a language model as a kind of classiÔ¨Åer or labelerfor a speciÔ¨Åc task. For example we might train the model to be a sentiment classiÔ¨Åer.We do this by adding extra neural circuitry (an extrahead) after the top layer of themodel. This classiÔ¨Åcation head takes as input some of the top layer embeddings ofthe transformer and produces as output a classiÔ¨Åcation. In this method, most com-monly used with masked language models like BERT, we freeze the entire pretrainedmodel and only train the classiÔ¨Åcation head on some new data, usually labeled withsome class that we want to predict.Finally, in Chapter 12 we‚Äôll introduce a fourth kind of Ô¨Ånetuning, that is a cru-cial component of the largest language models:supervised Ô¨ÅnetuningorSFT. SFTis often used forinstruction Ô¨Ånetuning, in which we want a pretrained languagemodel to learn to follow text instructions, for example to answer questions or followa command to write something. Here we create a dataset of prompts and desiredresponses (for example questions and their answers, or commands and their ful-Ô¨Ållments), and we train the language model using the normal cross-entropy loss topredict each token in the instruction prompt iteratively, essentially training it to pro-duce the desired response from the command in the prompt. It‚Äôs called supervisedbecause unlike in pretraining, where we just take any data and predict the words init, we build the special Ô¨Ånetuning dataset by hand, creating supervised responses toeach command.Often everything that happens after pretraining is lumped together aspost-training;we‚Äôll discuss the various parts of post-training in Chapter 12 and Chapter 13.10.4 Evaluating Large Language ModelsPerplexityAs we Ô¨Årst saw in Chapter 3, one way to evaluate language models isto measure how well they predict unseen text. Intuitively, good models are those thatassign higher probabilities to unseen data (are less surprised when encountering thenew words).We instantiate this intuition by usingperplexityto measure the quality of aperplexitylanguage model. Recall from page??that the perplexity of a modelqon an unseentest set is the inverse probability thatqassigns to the test set, normalized by the testset length. For a test set ofntokensw1:n, the perplexity isPerplexityq(w1:n)=Pq(w1:n) 1n=ns1Pq(w1:n)(10.7)To visualize how perplexity can be computed as a function of the probabilities the20CHAPTER7‚Ä¢LARGELANGUAGEMODELSsmaller the longer the text is; this is clear from the chain rule, since if we are mul-tiplying more probabilities, and each probability by deÔ¨Ånition is less than zero, theproduct will get smaller and smaller. So it‚Äôs useful to have a metric that is per-token,normalized by length, so we could compare across texts of different lengths.A function of probability calledperplexityis such a length-normalized metric.perplexityRecall from page??that the perplexity of a modelqon an unseen test set is theinverse probability thatqassigns to the test set (one over the probability of the testset), normalized by the test set length in tokens. For a test set ofntokensw1:n, theperplexity isPerplexityq(w1:n)=Pq(w1:n) 1n=ns1Pq(w1:n)(7.10)To visualize how perplexity can be computed as a function of the probabilities theLM computes for each new word, we can use the chain rule to expand the computa-tion of probability of the test set:Perplexityq(w1:n)=nvuutnYi=11Pq(wi|w<i)(7.11)Note that because of the inverse in Eq.7.10, the higher the probability of the wordsequence, the lower the perplexity. Thus thethe lower the perplexity of a model onthe data, the better the model. Minimizing perplexity is equivalent to maximizingthe test set probability according to the language model. Why does perplexity usethe inverse probability? The inverse arises from the original deÔ¨Ånition of perplexityfrom cross-entropy rate in information theory; for those interested, the explanationis in Section??. Meanwhile, we just have to remember that perplexity has an inverserelationship with probability.One caveat: because perplexity depends on the number of tokensnin a text, itis very sensitive to differences in the tokenization algorithm. That means that it‚Äôshard to exactly compare perplexities produced by two language models if they havevery different tokenizers. For this reason perplexity is best used when comparinglanguage models that use the same tokenizer.7.6.2 Downstream tasks: Reasoning and world knowledgePerplexity measures one kind of accuracy: accuracy at predicting words. But thereare other kinds of accuracy. For each of the downstream tasks we want to applyour language model, like question answering, machine translation, or reasoning,we could measure the accuracy at those tasks. We‚Äôll have further discussion ofthese task-speciÔ¨Åc evaluations in future chapters; machine translation in Chapter 12,information retrieval in Chapter 11, and speech recognition in Chapter 15.Here we brieÔ¨Çy introduce one such metric: a mechanism for measuring accu-racy in answering questions, focusing on multiple-choice questions. This dataset isMMLU(Massive Multitask Language Understanding), a commonly-used dataset ofMMLU15,908 knowledge and reasoning questions in 57 areas including medicine, mathe-matics, computer science, law, and others. Accuracy at answering these multiple-choice questions can be a useful proxy for the model‚Äôs ability to reason, and itsfactual knowledge.Perplexity‚Ä¢The higher the probability of the word sequence, the lower the perplexity.‚Ä¢Thus the lower the perplexity of a model on the data, the better the model. ‚Ä¢Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.  Many other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups. Large Language ModelsEthical and Safety Issues in Large Language ModelsHallucination
Privacy
Abuse and Toxicity
Lots moreHarm (suggesting dangerous actions)Fraud Emotional dependenceBiasMary Shelley's FrankensteinCentered on the problem of creating artificial agents without considering ethical and humanistic concerns. 22CHAPTER7‚Ä¢LARGELANGUAGEMODELSsizes. Big models also use more energy, and we prefer models that use less energy,both to reduce the environmental impact of the model and to reduce the Ô¨Ånancialcost of building or deploying it. We can target our evaluation to these factors bymeasuring performance normalized to a given compute or memory budget. We canalso directly measure the energy usage of our model in kWh or in kilograms of CO2emitted (Strubell et al.,2019;Henderson et al.,2020;Liang et al.,2023).Another feature that a language model evaluation can measure is fairness. Weknow that language models are biased, exhibiting gendered and racial stereotypes,or decreased performance for language from or about certain demographics groups.There are language model evaluation benchmarks that measure the strength of thesebiases, such as StereoSet (Nadeem et al.,2021), RealToxicityPrompts (Gehmanet al.,2020), and BBQ (Parrish et al.,2022) among many others. We also wantlanguage models whose performance is equally fair to different groups. For exam-ple, we could choose an evaluation that is fair in a Rawlsian sense by maximizingthe welfare of the worst-off group (Rawls,2001;Hashimoto et al.,2018;Sagawaet al.,2020).Finally, there are many kinds of leaderboards like Dynabench (Kiela et al.,2021)and general evaluation protocols like HELM (Liang et al.,2023); we will return tothese in later chapters when we introduce evaluation metrics for speciÔ¨Åc tasks likequestion answering and information retrieval.7.7 Ethical and Safety Issues with Language ModelsEthical and safety issues have been key to how we think about designing artiÔ¨Åcialagents since well before we had large language models. Mary Shelley (depictedbelow) centered her novelFrankensteinaround the problem of creating artiÔ¨Åcialagents without considering ethical and humanistic concerns.Large language models can be unsafe in many ways. For example, LLMs
are prone to saying things that are false,a problem calledhallucination. Languagehallucinationmodels are trained to generate text that is pre-dictable and coherent, but the training algo-rithms we have seen so far don‚Äôt have anyway to enforce that the text that is generatedis correct or true. This causes enormous prob-lems for any application where the facts mat-ter! A related symptom is that language mod-els cansuggest unsafe actions, for exampledirectly suggesting that users do dangerous orillegal things like harming themselves or oth-ers. If users seek information from languagemodels in safety-critical situations like askingmedical advice, or in emergency situations, orwhen indicating the intentions of self-harm,incorrect advice can be dangerous and even life-threatening. Again, this problempredates large language models For example (Bickmore et al.,2018) gave partic-ipants medical problems to pose to three pre-LLM commercial dialogue systems(Siri, Alexa, Google Assistant) and asked them to determine an action to take basedon the system responses; many of the proposed actions, if actually taken, would haveLarge Language ModelsEthical and Safety Issues in Large Language Models