Information Retrieval and RAGThe Information Retrieval TaskInformation retrievalUser has an information needAnd has some collection of documentsUser wants to find a relevantdocument‚Ä¢a document (or documents)‚Ä¢in the collection‚Ä¢that satisfy their needWeb search
Not just the webSearching our emailSearchingcorporatedocumentsSearching personal medical recordsAnd also, part of LLMs‚Ä¢retrieval-augmented generation (RAG)In most cases we do "ranked retrieval"The retriever returns top-k documentsTheseare rankedWe can show the user these, or some subset.Ad-hoc retrieval
DocumentDocumentDocumentDocumentDocumentDocumentQuery ProcessingDocument Processing & IndexingSearchMaximize:Document Relevance ScoreDocumentDocumentDocumentDocumentDocumentRanked DocumentsDocumentqueryDocumentIndexQueryVectordocument collectionDocument Relevance ScoreGoalistoassign a score to each document for whether it meets the user's information needInstead, we just approximate this by the textual similarity between the query and the document.Two architecturesSparse retrieval‚ó¶represent queryanddocasvectorsofwordcounts‚ó¶weighted by tf-idf,BM25Denseretrieval‚ó¶Use LLM to represent queryanddocas embeddingsIn both cases, similarity is dot product or cosine between query and document representations Information Retrieval and RAGThe Information Retrieval TaskInformation Retrieval and RAGSparse retrieval: the vector model ofIRThe vector space model of IRRepresentadocument as a vector of counts of the words it contains. Gerard Salton, 1971Bag-of-words modelititititititIIII
Iloverecommendmoviethethethetheto
totoand
andandseenseenyetwouldwithwhowhimsical
whilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!adventure   and fairy   genre   great   have    humor   I   itsatirical   seen    sweet   the times   to  whimsical   would   yet ‚Ä¶13111115   6   121413111‚Ä¶   Vector representation of that doc[1 3 1 1 1 1 1 5 6 1 2 1 4 1 3 1 1 1]Term-document matrix6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that are6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that areEach document is represented by a vector of wordsVisualizing document vectors6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that areThe two dimensional space [battle, fool]Vectors are the basis of information retrieval6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that areVectors are similar for the two comediesBut comedies are different than the other two  Comedies have more fools and wit and fewer battles.Vector representations of queries and documentsSuppose we are looking for a witty fool play:Query = "fool wit"6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that are0011QueryChoose the document that is most similar to the queryWhich of d1, d2, d3, d4 is most similar to q?6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that are0011Queryqd1d2d4d3Similarity methods are variants of dot productThe dot product is q ‚àô dscore (q,d1)  = q ‚àô d1 = 6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that are0011Queryqd1d2d4d3In fact we use cosine11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.In fact we use cosine11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.Information Retrieval and RAGSparse retrieval: the vector model of IRInformation Retrieval and RAGTF-IDFBut raw frequency is a bad representation‚Ä¢The co-occurrence matrices we have seen represent each cell by word frequencies.‚Ä¢Frequency is clearly useful; if sugar appears a lot near apricot, that's useful information.‚Ä¢But overly frequent words like the, it, or they are not very informative about the context‚Ä¢It's a paradox! How can we balance these two conflicting constraints? Two common solutions for word weightingtf-idf:     tf-idf value for word t in document d:PMI: (Pointwise mutual information)‚ó¶PMIùíòùüè,ùíòùüê=ùíçùíêùíàùíë(ùíòùüè,ùíòùüê)ùíëùíòùüèùíë(ùíòùüê) 14CHAPTER6‚Ä¢VECTORSEMANTICSCollection FrequencyDocument FrequencyRomeo1131action11331We assign importance to these more discriminative words likeRomeoviatheinverse document frequencyoridfterm weight(Sparck Jones, 1972).idfThe idf is deÔ¨Åned using the fractionN/dft, whereNis the total number ofdocuments in the collection, and dftis the number of documents in whichtermtoccurs. The fewer documents in which a term occurs, the higher thisweight. The lowest weight of 1 is assigned to terms that occur in all thedocuments. It‚Äôs usually clear what counts as a document: in Shakespearewe would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaperarticles, the document is a single article. Occasionally your corpus mightnot have appropriate document divisions and you might need to break up thecorpus into documents yourself for the purposes of computing idf.Because of the large number of documents in many collections, this mea-sure is usually squashed with a log function. The resulting deÔ¨Ånition for in-verse document frequency (idf) is thusidft=log10‚úìNdft‚óÜ(6.12)Here are some idf values for some words in the Shakespeare corpus, rangingfrom extremely informative words which occur in only one play likeRomeo, tothose that occur in a few likesaladorFalstaff, to those which are very common likefoolor so common as to be completely non-discriminative since they occur in all 37plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.074fool360.012good370sweet370Thetf-idfweighting of the value for wordtin documentd,wt,dthus combinestf-idfterm frequency with idf:wt,d=tft,d‚á•idft(6.13)Fig.6.8applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2.Note that the tf-idf values for the dimension corresponding to the wordgoodhavenow all become 0; since this word appears in every document, the tf-idf algorithmleads it to be ignored in any comparison of the plays. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.The tf-idf weighting is by far the dominant way of weighting co-occurrence ma-trices in information retrieval, but also plays a role in many other aspects of natural3Sweetwas one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).Words like "the" or "it" have very low idfSee if words like "good" appear more often with "great" than we would expect by chanceTerm frequency (tf) in the tf-idf algorithmWe could imagine using raw count: tft,d = count(t,d)But instead of using raw count, we usually squash a bit:12CHAPTER6‚Ä¢VECTORSEMANTICS ANDEMBEDDINGSis not the best measure of association between words. Raw frequency is very skewedand not very discriminative. If we want to know what kinds of contexts are sharedbycherryandstrawberrybut not bydigitalandinformation, we‚Äôre not going to getgood discrimination from words likethe,it, orthey, which occur frequently withall sorts of words and aren‚Äôt informative about any particular word. We saw thisalso in Fig.6.3for the Shakespeare corpus; the dimension for the wordgoodis notvery discriminative between plays;goodis simply a frequent word and has roughlyequivalent high frequencies in each of the plays.It‚Äôs a bit of a paradox. Words that occur nearby frequently (maybepienearbycherry) are more important than words that only appear once or twice. Yet wordsthat are too frequent‚Äîubiquitous, liketheorgood‚Äî are unimportant. How can webalance these two conÔ¨Çicting constraints?There are two common solutions to this problem: in this section we‚Äôll describethetf-idfweighting, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf weighting(the ‚Äò-‚Äô here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The Ô¨Årst is theterm frequency(Luhn,1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn‚Äôt make that word 100 times more likely to be relevant to the meaning of thedocument. We also need to do something special with counts of 0, since we can‚Äôttake the log of 0.2tft,d=(1+log10count(t,d)if count(t,d)>00 otherwise(6.12)If we use log weighting, terms which occur 0 times in a document would have tf=0,1 times in a document tf=1+log10(1)=1+0=1, 10 times in a document tf=1+log10(10)=2, 100 times tf=1+log10(100)=3, 1000 times tf=4, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren‚Äôt as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare‚Äôs 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to Ô¨Ånd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action113312We can also use this alternative formulation, which we have used in earlier editions: tft,d=log10(count(t,d)+1)Document frequency (df)dft is the number of documents t occurs in.(note this is not collection frequency: total count across all documents)"Romeo" is very distinctive for one Shakespeare play:12CHAPTER6‚Ä¢VECTORSEMANTICS ANDEMBEDDINGSthat are too frequent‚Äîubiquitous, liketheorgood‚Äî are unimportant. How can webalance these two conÔ¨Çicting constraints?There are two common solutions to this problem: in this section we‚Äôll describethetf-idfalgorithm, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf algorithm(the ‚Äò-‚Äô here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The Ô¨Årst is theterm frequency(Luhn, 1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn‚Äôt make that word 100 times more likely to be relevant to the meaning of thedocument. Because we can‚Äôt take the log of 0, we normally add 1 to the count:2tft,d=log10(count(t,d)+1)(6.12)If we use log weighting, terms which occur 0 times in a document would havetf=log10(1)=0, 10 times in a document tf=log10(11)=1.4, 100 times tf=log10(101)=2.004, 1000 times tf=3.00044, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren‚Äôt as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare‚Äôs 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to Ô¨Ånd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action11331We emphasize discriminative words likeRomeovia theinverse document fre-quencyoridfterm weight(Sparck Jones, 1972). The idf is deÔ¨Åned using the frac-idftionN/dft, whereNis the total number of documents in the collection, and dftisthe number of documents in which termtoccurs. The fewer documents in which aterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms thatoccur in all the documents. It‚Äôs usually clear what counts as a document: in Shake-speare we would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,the document is a single article. Occasionally your corpus might not have appropri-ate document divisions and you might need to break up the corpus into documentsyourself for the purposes of computing idf.2Or we can use this alternative: tft,d=‚á¢1+log10count(t,d)if count(t,d)>00 otherwiseInverse document frequency (idf)6.5‚Ä¢TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deÔ¨Ånition for inversedocument frequency (idf) is thusidft=log10‚úìNdft‚óÜ(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deÔ¨Åned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d‚á•idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It‚Äôs also a great baseline, the simple thing to try Ô¨Årst. We‚Äôll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.5‚Ä¢TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deÔ¨Ånition for inversedocument frequency (idf) is thusidft=log10‚úìNdft‚óÜ(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deÔ¨Åned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d‚á•idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It‚Äôs also a great baseline, the simple thing to try Ô¨Årst. We‚Äôll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).N is the total number of documents in the collectionWhat is a document?Could be a play or a Wikipedia articleBut for the purposes of tf-idf, documents can be anything; we often call each paragraph a document!Final tf-idf weighted value for a wordRaw counts:tf-idf:6.5‚Ä¢TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deÔ¨Ånition for inversedocument frequency (idf) is thusidft=log10‚úìNdft‚óÜ(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deÔ¨Åned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d‚á•idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It‚Äôs also a great baseline, the simple thing to try Ô¨Årst. We‚Äôll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.3‚Ä¢WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the Ô¨Årstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they Ô¨Åt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the Ô¨Årst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the Ô¨Årst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we‚Äôvearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similardocuments for the task of documentinformation retrieval. Two documents that are14CHAPTER6‚Ä¢VECTORSEMANTICS ANDEMBEDDINGSAs You Like It Twelfth Night Julius Caesar Henry Vbattle0.246 0 0.454 0.520good00 0 0fool0.030 0.033 0.0012 0.0019wit0.085 0.081 0.048 0.054Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.085 value forwitinAs You Like Itisthe product of tf=1+log10(20)=2.301 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.6.6 Pointwise Mutual Information (PMI)An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-mation), is used for term-term-matrices, when the vector dimensions correspond towords rather than documents. PPMI draws on the intuition that the best way to weighthe association between two words is to ask how muchmorethe two words co-occurin our corpus than we would have a priori expected them to appear by chance.Pointwise mutual information(Fano,1961)4is one of the most important con-pointwisemutualinformationcepts in NLP. It is a measure of how often two eventsxandyoccur, compared withwhat we would expect if they were independent:I(x,y)=log2P(x,y)P(x)P(y)(6.16)The pointwise mutual information between a target wordwand a context wordc(Church and Hanks1989,Church and Hanks1990) is then deÔ¨Åned as:PMI(w,c)=log2P(w,c)P(w)P(c)(6.17)The numerator tells us how often we observed the two words together (assumingwe compute probability by using the MLE). The denominator tells us how oftenwe wouldexpectthe two words to co-occur assuming they each occurred indepen-dently; recall that the probability of two independent events both occurring is justthe product of the probabilities of the two events. Thus, the ratio gives us an esti-mate of how much more the two words co-occur than we expect by chance. PMI isa useful tool whenever we need to Ô¨Ånd words that are strongly associated.PMI values range from negative to positive inÔ¨Ånity. But negative PMI values(which imply things are co-occurringless oftenthan we would expect by chance)tend to be unreliable unless our corpora are enormous. To distinguish whethertwo words whose individual probability is each 10 6occur together less often thanchance, we would need to be certain that the probability of the two occurring to-gether is signiÔ¨Åcantly less than 10 12, and this kind of granularity would require anenormous corpus. Furthermore it‚Äôs not clear whether it‚Äôs even possible to evaluatesuch scores of ‚Äòunrelatedness‚Äô with human judgments. For this reason it is more4PMI is based on themutual informationbetween two random variablesXandY, deÔ¨Åned as:I(X,Y)=XxXyP(x,y)log2P(x,y)P(x)P(y)(6.15)In a confusion of terminology, Fano used the phrasemutual informationto refer to what we now callpointwise mutual informationand the phraseexpectation of the mutual informationfor what we now callmutual informationInformation Retrieval and RAGTF-IDFInformation Retrieval and RAGTF-IDF: a worked exampleCosine11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.TF-IDF weighted cosine11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.TF-IDF weighted cosine11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.12CHAPTER6‚Ä¢VECTORSEMANTICS ANDEMBEDDINGSis not the best measure of association between words. Raw frequency is very skewedand not very discriminative. If we want to know what kinds of contexts are sharedbycherryandstrawberrybut not bydigitalandinformation, we‚Äôre not going to getgood discrimination from words likethe,it, orthey, which occur frequently withall sorts of words and aren‚Äôt informative about any particular word. We saw thisalso in Fig.6.3for the Shakespeare corpus; the dimension for the wordgoodis notvery discriminative between plays;goodis simply a frequent word and has roughlyequivalent high frequencies in each of the plays.It‚Äôs a bit of a paradox. Words that occur nearby frequently (maybepienearbycherry) are more important than words that only appear once or twice. Yet wordsthat are too frequent‚Äîubiquitous, liketheorgood‚Äî are unimportant. How can webalance these two conÔ¨Çicting constraints?There are two common solutions to this problem: in this section we‚Äôll describethetf-idfweighting, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf weighting(the ‚Äò-‚Äô here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The Ô¨Årst is theterm frequency(Luhn,1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn‚Äôt make that word 100 times more likely to be relevant to the meaning of thedocument. We also need to do something special with counts of 0, since we can‚Äôttake the log of 0.2tft,d=(1+log10count(t,d)if count(t,d)>00 otherwise(6.12)If we use log weighting, terms which occur 0 times in a document would have tf=0,1 times in a document tf=1+log10(1)=1+0=1, 10 times in a document tf=1+log10(10)=2, 100 times tf=1+log10(100)=3, 1000 times tf=4, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren‚Äôt as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare‚Äôs 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to Ô¨Ånd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action113312We can also use this alternative formulation, which we have used in earlier editions: tft,d=log10(count(t,d)+1)6.5‚Ä¢TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deÔ¨Ånition for inversedocument frequency (idf) is thusidft=log10‚úìNdft‚óÜ(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deÔ¨Åned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d‚á•idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It‚Äôs also a great baseline, the simple thing to try Ô¨Årst. We‚Äôll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).TF-IDF nano-example11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.TF-IDF nano-exampleWordCounttf1+log10(c)dfidflog10N/df)tf-idftf x idfnormalizedsweet1nurse0love1how0sorrow0is011.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.|q| = ‚àöTF-IDF nano-exampleWordCounttf1+log10(c)dfidflog10N/df)tf-idftf x idfnormalizedsweet1130.1250.1250.383nurse0love1120.3010.3010.924how0sorrow0is011.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.|q| = ‚àö(.1252 + .3012) = .325TF-IDF nano-exampleWordCounttf1+log10(c)dfidflog10N/df)tf-idftf x idfnormalized√ó qsweet1nurse0love0how0sorrow1is011.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.|q| = ‚àöTF-IDF nano-exampleWordCnttf1+log10(c)dfidflog10N/df)tf-idftf x idfnormalized√ó qsweet1130.125.125.2030.0779nurse0love0how0sorrow111.6020.602.9790is011.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.|q| = ‚àö(.1252 + .6022) = .615TF-IDF nano-exampleWordCnttf1+log10(c)dfidflog10N/df)tf-idftf x idfnormalized√ó qsweet1130.125.125.2030.0779nurse0love0how0sorrow111.6020.602.9790is011.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader..= 0.0779Final cosinescore(q,d1) =  0.747score(q,d2) = 0.0779‚ó¶d1 has both terms, including 2 instances of sweet‚ó¶d2 is missing one of the terms11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.(details inchapter)Information Retrieval and RAGTF-IDF: a worked exampleInformation Retrieval and RAGEfficiency: The Inverted IndexGoal: rank documents in D by their TF-IDF-weighted  cosines with query qDo we have to consider all documents in D?No!  We can ignore all documents that don't have any query words! They will have a cosine of 0!How do we efficiently find all documents that contain a query term qi?An index!  Which for historical reasons we call an inverted index!Inverted IndexTwo parts11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.Dictionary:howis lovenursesorrysweet Postings:√† 3√† 3√† 1√† 3√† 1 √† 4√† 2√† 1√† 2 √† 3Inverted Index Creation1. Sort by term and document11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.Dict:howis lovenursesorrowsweet Postings:√† 3√† 3√† 1√† 3√† 1 √† 4√† 2√† 1√† 2 √† 3Term Doc#sweet 1sweet 1nurse 1love 1sweet 2sorrow 2how 3sweet 3is 3love 3nurse 4Term Doc#how 3is 3love 1love 3nurse 1nurse 4sorrow 2sweet  2sweet 1sweet 1sweet 3√†√†√†2. Create linked postings listInverted IndexDicthowis lovenursesorrowsweet Postings√† 3√† 3√† 1√† 3√† 1 √† 4√† 2√† 1√† 2 √† 3So far this just tells us which documents to grab Next we need enough information to compute tf-idf:For each term t in vocabulary:‚ó¶The dft (document frequency) of word tFor each term t in each document d:‚ó¶The term frequency or count(t,d) of word i in doc d11.1‚Ä¢INFORMATIONRETRIEV AL7where|davg|is the length of the average document. Whenkis 0, BM25 reverts tono use of term frequency, just a binary selection of terms in the query (plus idf).A largekresults in raw term frequency (plus idf).branges from 1 (scaling bydocument length) to 0 (no length scaling).Manning et al.(2008) suggest reasonablevalues are k = [1.2,2] and b = 0.75.Kamphuis et al.(2020) is a useful summary ofthe many minor variants of BM25.Stop wordsIn the past it was common to remove high-frequency words from boththe query and document before representing them. The list of such high-frequencywords to be removed is called astop list. The intuition is that high-frequency termsstop list(often function words likethe,a,to) carry little semantic weight and may not helpwith retrieval, and can also help shrink the inverted index Ô¨Åles we describe below.The downside of using a stop list is that it makes it difÔ¨Åcult to search for phrasesthat contain words in the stop list. For example, common stop lists would reduce thephraseto be or not to beto the phrasenot. In modern IR systems, the use of stop listsis much less common, partly due to improved efÔ¨Åciency and partly because muchof their function is already handled by IDF weighting, which downweights functionwords that occur in every document. Nonetheless, stop word removal is occasionallyuseful in various NLP tasks so is worth keeping in mind.11.1.3 Inverted IndexIn order to compute scores, we need to efÔ¨Åciently Ô¨Ånd documents that contain wordsin the query. (Any document that contains none of the query terms will have a scoreof 0 and can be ignored.) The basic search problem in IR is thus to Ô¨Ånd all documentsd2Cthat contain a termq2Q.The data structure for this task is theinverted index, which we use for mak-inverted indexing this search efÔ¨Åcient, and also conveniently storing useful information like thedocument frequency and the count of each term in each document.An inverted index, given a query term, gives a list of documents that contain theterm. It consists of two parts, adictionaryand thepostings. The dictionary is a listpostingsof terms (designed to be efÔ¨Åciently accessed), each pointing to apostings listfor theterm. A postings list is the list of document IDs associated with each term, whichcan also contain information like the term frequency or even the exact positions ofterms in the document. The dictionary can also store the document frequency foreach term. For example, a simple inverted index for our 4 sample documents above,with each word containing its document frequency in{}, and a pointer to a postingslist that contains document IDs and term counts in [], might look like the following:how{1}!3[1]is{1}!3[1]love{2}!1[1]!3[1]nurse{2}!1[1]!4[1]sorry{1}!2[1]sweet{3}!1[2]!2[1]!3[1]Given a list of terms in query, we can very efÔ¨Åciently get lists of all candidatedocuments, together with the information necessary to compute the tf-idf scores weneed.There are alternatives to the inverted index. For the question-answering domainof Ô¨Ånding Wikipedia pages to match a user query,Chen et al.(2017) show thatindexing based on bigrams works better than unigrams, and use efÔ¨Åcient hashingalgorithms rather than the inverted index to make the search efÔ¨Åcient.11.1‚Ä¢INFORMATIONRETRIEV AL5Thetf-idfvalue for wordtin documentdis then the product of term frequencytft,dand IDF:tf-idf(t,d)=tft,d¬∑idft(11.6)11.1.2 Document ScoringWe score documentdby the cosine of its vectordwith the query vectorq:score(q,d)=cos(q,d)=q¬∑d|q||d|(11.7)Another way to think of the cosine computation is as the dot product of unit vectors;we Ô¨Årst normalize both the query and document vector to unit vectors, by dividingby their lengths, and then take the dot product:score(q,d)=cos(q,d)=q|q|¬∑d|d|(11.8)We can spell out Eq.11.8, using the tf-idf values and spelling out the dot product asa sum of products:score(q,d)=Xt2qtf-idf(t,q)qPqi2qtf-idf2(qi,q)¬∑tf-idf(t,d)qPdi2dtf-idf2(di,d)(11.9)Now let‚Äôs use Eq.11.9to walk through an example of a tiny query against acollection of 4 nano documents, computing tf-idf values and seeing the rank of thedocuments. We‚Äôll assume all words in the following query and documents are down-cased and punctuation is removed:Query:sweet loveDoc 1:Sweet sweet nurse! Love?Doc 2:Sweet sorrowDoc 3:How sweet is love?Doc 4:Nurse!Fig.11.2shows the computation of the tf-idf cosine between the query and Doc-ument 1, and the query and Document 2. The cosine is the normalized dot productof tf-idf values, so for the normalization we must need to compute the documentvector lengths|q|,|d1|, and|d2|for the query and the Ô¨Årst two documents usingEq.11.4, Eq.11.5, Eq.11.6, and Eq.11.9(computations for Documents 3 and 4 arealso needed but are left as an exercise for the reader). The dot product between thevectors is the sum over dimensions of the product, for each dimension, of the valuesof the two tf-idf vectors for that dimension. This product is only non-zero whereboth the query and document have non-zero values, so for this example, in whichonlysweetandlovehave non-zero values in the query, the dot product will be thesum of the products of those elements of each vector.Document 1 has a higher cosine with the query (0.747) than Document 2 haswith the query (0.0779), and so the tf-idf cosine model would rank Document 1above Document 2. This ranking is intuitive given the vector space model, sinceDocument 1 has both terms including two instances ofsweet, while Document 2 ismissing one of the terms. We leave the computation for Documents 3 and 4 as anexercise for the reader.dftfDoc#Information Retrieval and RAGEfficiency: The Inverted IndexInformation Retrieval and RAGEvaluation of IRPrecision and Recalltrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fnPrecision: % of selected items that are correctRecall: % of correct items that are selectedWe saw these already for classificationPrecision and Recall for IRUser makes an information requestEvery document in collection is either:‚Ä¢Relevant to the user‚Ä¢Not relevant to the userThe system retrieves a ranked set of documentsPrecision for IRPrecision = % of retrieved documents that are relevantSystem retrieves two kinds of documents relevant documents irrelevant documents   |relevant retrieved docs| Precision     =    ----------------------------------------  |relevant retrieved docs| + |irrelevant retrieved docs| Recall for IRRecall = % of relevant documents that are retrievedRecall = |retrieved relevant documents|  -------------------------------    |all relevant document|Precision and Recall aren't enoughThis is ranked retrieval‚Ä¢Given two ranked retrieval systems‚Ä¢We want a metric that prefers the one that ranks relevant documents higherWe need to adapt precision and recall!‚Ä¢to be sensitive to where in the ranking the relevant document occurRank-specificprecision and recallExample:‚Ä¢25 documents‚Ä¢9 are relevant‚Ä¢If we return all 25P=.36, R= 1.0‚Ä¢Suppose we just return one (and it is relevant)?8CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS11.1.4 Evaluation of Information-Retrieval SystemsWe measure the performance of ranked retrieval systems using the sameprecisionandrecallmetrics we have been using. We make the assumption that each docu-ment returned by the IR system is eitherrelevantto our purposes ornot relevant.Precision is the fraction of the returned documents that are relevant, and recall is thefraction of all relevant documents that are returned. More formally, let‚Äôs assume asystem returnsTranked documents in response to an information request, a subsetRof these are relevant, a disjoint subset,N, are the remaining irrelevant documents,andUdocuments in the collection as a whole are relevant to this request. Precisionand recall are then deÔ¨Åned as:Precision=|R||T|Recall=|R||U|(11.13)Unfortunately, these metrics don‚Äôt adequately measure the performance of a systemthatranksthe documents it returns. If we are comparing the performance of tworanked retrieval systems, we need a metric that prefers the one that ranks the relevantdocuments higher. We need to adapt precision and recall to capture how well asystem does at putting relevant documents higher in the ranking.Rank Judgment PrecisionRankRecallRank1 R 1.0 .112 N .50 .113 R .66 .224 N .50 .225 R .60 .336 R .66 .447 N .57 .448 R .63 .559 N .55 .5510 N .50 .5511 R .55 .6612 N .50 .6613 N .46 .6614 N .43 .6615 R .47 .7716 N .44 .7717 N .44 .7718 R .44 .8819 N .42 .8820 N .40 .8821 N .38 .8822 N .36 .8823 N .35 .8824 N .33 .8825 R .36 1.0Figure 11.3Rank-speciÔ¨Åc precision and recall values calculated as we proceed downthrough a set of ranked documents (assuming the collection has 9 relevant documents).Let‚Äôs turn to an example. Assume the table in Fig.11.3gives rank-speciÔ¨Åc pre-cision and recall values calculated as we proceed down through a set of ranked doc-uments for a particular query; the precisions are the fraction of relevant documentsseen at a given rank, and recalls the fraction of relevant documents found at the sameRank-specificprecision and recallRecall is non-decreasing‚Ä¢Relevant docs increase recall‚Ä¢Non-relevant docs don'tPrecision jumps up and down‚Ä¢increasing for relevant docs‚Ä¢decreasing otherwise. 8CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS11.1.4 Evaluation of Information-Retrieval SystemsWe measure the performance of ranked retrieval systems using the sameprecisionandrecallmetrics we have been using. We make the assumption that each docu-ment returned by the IR system is eitherrelevantto our purposes ornot relevant.Precision is the fraction of the returned documents that are relevant, and recall is thefraction of all relevant documents that are returned. More formally, let‚Äôs assume asystem returnsTranked documents in response to an information request, a subsetRof these are relevant, a disjoint subset,N, are the remaining irrelevant documents,andUdocuments in the collection as a whole are relevant to this request. Precisionand recall are then deÔ¨Åned as:Precision=|R||T|Recall=|R||U|(11.13)Unfortunately, these metrics don‚Äôt adequately measure the performance of a systemthatranksthe documents it returns. If we are comparing the performance of tworanked retrieval systems, we need a metric that prefers the one that ranks the relevantdocuments higher. We need to adapt precision and recall to capture how well asystem does at putting relevant documents higher in the ranking.Rank Judgment PrecisionRankRecallRank1 R 1.0 .112 N .50 .113 R .66 .224 N .50 .225 R .60 .336 R .66 .447 N .57 .448 R .63 .559 N .55 .5510 N .50 .5511 R .55 .6612 N .50 .6613 N .46 .6614 N .43 .6615 R .47 .7716 N .44 .7717 N .44 .7718 R .44 .8819 N .42 .8820 N .40 .8821 N .38 .8822 N .36 .8823 N .35 .8824 N .33 .8825 R .36 1.0Figure 11.3Rank-speciÔ¨Åc precision and recall values calculated as we proceed downthrough a set of ranked documents (assuming the collection has 9 relevant documents).Let‚Äôs turn to an example. Assume the table in Fig.11.3gives rank-speciÔ¨Åc pre-cision and recall values calculated as we proceed down through a set of ranked doc-uments for a particular query; the precisions are the fraction of relevant documentsseen at a given rank, and recalls the fraction of relevant documents found at the sameThe precision-recall curve (for one query)8CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS11.1.4 Evaluation of Information-Retrieval SystemsWe measure the performance of ranked retrieval systems using the sameprecisionandrecallmetrics we have been using. We make the assumption that each docu-ment returned by the IR system is eitherrelevantto our purposes ornot relevant.Precision is the fraction of the returned documents that are relevant, and recall is thefraction of all relevant documents that are returned. More formally, let‚Äôs assume asystem returnsTranked documents in response to an information request, a subsetRof these are relevant, a disjoint subset,N, are the remaining irrelevant documents,andUdocuments in the collection as a whole are relevant to this request. Precisionand recall are then deÔ¨Åned as:Precision=|R||T|Recall=|R||U|(11.13)Unfortunately, these metrics don‚Äôt adequately measure the performance of a systemthatranksthe documents it returns. If we are comparing the performance of tworanked retrieval systems, we need a metric that prefers the one that ranks the relevantdocuments higher. We need to adapt precision and recall to capture how well asystem does at putting relevant documents higher in the ranking.Rank Judgment PrecisionRankRecallRank1 R 1.0 .112 N .50 .113 R .66 .224 N .50 .225 R .60 .336 R .66 .447 N .57 .448 R .63 .559 N .55 .5510 N .50 .5511 R .55 .6612 N .50 .6613 N .46 .6614 N .43 .6615 R .47 .7716 N .44 .7717 N .44 .7718 R .44 .8819 N .42 .8820 N .40 .8821 N .38 .8822 N .36 .8823 N .35 .8824 N .33 .8825 R .36 1.0Figure 11.3Rank-speciÔ¨Åc precision and recall values calculated as we proceed downthrough a set of ranked documents (assuming the collection has 9 relevant documents).Let‚Äôs turn to an example. Assume the table in Fig.11.3gives rank-speciÔ¨Åc pre-cision and recall values calculated as we proceed down through a set of ranked doc-uments for a particular query; the precisions are the fraction of relevant documentsseen at a given rank, and recalls the fraction of relevant documents found at the same11.1‚Ä¢INFORMATIONRETRIEV AL9
Figure 11.4The precision recall curve for the data in table11.3.rank. The recall measures in this example are based on this query having 9 relevantdocuments in the collection as a whole.Note that recall is non-decreasing; when a relevant document is encountered,recall increases, and when a non-relevant document is found it remains unchanged.Precision, on the other hand, jumps up and down, increasing when relevant doc-uments are found, and decreasing otherwise. The most common way to visualizeprecision and recall is to plot precision against recall in aprecision-recall curve,precision-recallcurvelike the one shown in Fig.11.4for the data in table11.3.Fig.11.4shows the values for a single query. But we‚Äôll need to combine valuesfor all the queries, and in a way that lets us compare one system to another. One wayof doing this is to plot averaged precision values at 11 Ô¨Åxed levels of recall (0 to 100,in steps of 10). Since we‚Äôre not likely to have datapoints at these exact levels, weuseinterpolated precisionvalues for the 11 recall values from the data points we dointerpolatedprecisionhave. We can accomplish this by choosing the maximum precision value achievedat any level of recall at or above the one we‚Äôre calculating. In other words,IntPrecision(r)=maxi>=rPrecision(i)(11.14)This interpolation scheme not only lets us average performance over a set of queries,but also helps smooth over the irregular precision values in the original data. It isdesigned to give systems the beneÔ¨Åt of the doubt by assigning the maximum preci-sion value achieved at higher levels of recall from the one being measured. Fig.11.5and Fig.11.6show the resulting interpolated data points from our example.Given curves such as that in Fig.11.6we can compare two systems or approachesby comparing their curves. Clearly, curves that are higher in precision across allrecall values are preferred. However, these curves can also provide insight into theoverall behavior of a system. Systems that are higher in precision toward the leftmay favor precision over recall, while systems that are more geared towards recallwill be higher at higher levels of recall (to the right).A second way to evaluate ranked retrieval ismean average precision(MAP),mean averageprecisionwhich provides a single metric that can be used to compare competing systems orapproaches. In this approach, we again descend through the ranked list of items,but now we note the precisiononlyat those points where a relevant item has beenencountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig.11.3). For a singleNeed a metric that aggregates over many queriesTwo common approaches‚Ä¢Mean Average Precision‚Ä¢Interpolated PrecisionMean Average PrecisionDescend through ranked itemsNote precision only if item is relevant‚Ä¢e.g. ranks 1, 3, 5, 6 but not 2 or 4:Precisionr(d) "ranked precision"8CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS11.1.4 Evaluation of Information-Retrieval SystemsWe measure the performance of ranked retrieval systems using the sameprecisionandrecallmetrics we have been using. We make the assumption that each docu-ment returned by the IR system is eitherrelevantto our purposes ornot relevant.Precision is the fraction of the returned documents that are relevant, and recall is thefraction of all relevant documents that are returned. More formally, let‚Äôs assume asystem returnsTranked documents in response to an information request, a subsetRof these are relevant, a disjoint subset,N, are the remaining irrelevant documents,andUdocuments in the collection as a whole are relevant to this request. Precisionand recall are then deÔ¨Åned as:Precision=|R||T|Recall=|R||U|(11.13)Unfortunately, these metrics don‚Äôt adequately measure the performance of a systemthatranksthe documents it returns. If we are comparing the performance of tworanked retrieval systems, we need a metric that prefers the one that ranks the relevantdocuments higher. We need to adapt precision and recall to capture how well asystem does at putting relevant documents higher in the ranking.Rank Judgment PrecisionRankRecallRank1 R 1.0 .112 N .50 .113 R .66 .224 N .50 .225 R .60 .336 R .66 .447 N .57 .448 R .63 .559 N .55 .5510 N .50 .5511 R .55 .6612 N .50 .6613 N .46 .6614 N .43 .6615 R .47 .7716 N .44 .7717 N .44 .7718 R .44 .8819 N .42 .8820 N .40 .8821 N .38 .8822 N .36 .8823 N .35 .8824 N .33 .8825 R .36 1.0Figure 11.3Rank-speciÔ¨Åc precision and recall values calculated as we proceed downthrough a set of ranked documents (assuming the collection has 9 relevant documents).Let‚Äôs turn to an example. Assume the table in Fig.11.3gives rank-speciÔ¨Åc pre-cision and recall values calculated as we proceed down through a set of ranked doc-uments for a particular query; the precisions are the fraction of relevant documentsseen at a given rank, and recalls the fraction of relevant documents found at the sameprecision at the rank doc d was found.Average PrecisionDescend through ranked itemsNote precision only if item is relevantTake the average of the ranked-precisions‚Ä¢Rr is the set of relevant documents at or above r‚Ä¢Precisionr(d) is precision measured at the rank at which document d was found.10CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELSInterpolated Precision Recall1.0 0.01.0 .10.66 .20.66 .30.66 .40.63 .50.55 .60.47 .70.44 .80.36 .90.36 1.0Figure 11.5Interpolated data points from Fig.11.3.
Interpolated Precision Recall Curve
00.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1RecallPrecision
Figure 11.6An 11 point interpolated precision-recall curve. Precision at each of the 11standard recall levels is interpolated for each query from the maximum at any higher level ofrecall. The original measured precision recall points are also shown.query, we average these individual precision measurements over the return set (upto some Ô¨Åxed cutoff). More formally, if we assume thatRris the set of relevantdocuments at or abover, then theaverage precision(AP) for a single query isAP=1|Rr|Xd2RrPrecisionr(d)(11.15)wherePrecisionr(d)is the precision measured at the rank at which documentdwasfound. For an ensemble of queriesQ, we then average over these averages, to getour Ô¨Ånal MAP measure:MAP=1|Q|Xq2QAP(q)(11.16)The MAP for the single query (hence = AP) in Fig.11.3is 0.6.Mean Average PrecisionFor a set of Q queriesMean Average Precision (MAP):10CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELSInterpolated Precision Recall1.0 0.01.0 .10.66 .20.66 .30.66 .40.63 .50.55 .60.47 .70.44 .80.36 .90.36 1.0Figure 11.5Interpolated data points from Fig.11.3.
Interpolated Precision Recall Curve
00.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1RecallPrecision
Figure 11.6An 11 point interpolated precision-recall curve. Precision at each of the 11standard recall levels is interpolated for each query from the maximum at any higher level ofrecall. The original measured precision recall points are also shown.query, we average these individual precision measurements over the return set (upto some Ô¨Åxed cutoff). More formally, if we assume thatRris the set of relevantdocuments at or abover, then theaverage precision(AP) for a single query isAP=1|Rr|Xd2RrPrecisionr(d)(11.15)wherePrecisionr(d)is the precision measured at the rank at which documentdwasfound. For an ensemble of queriesQ, we then average over these averages, to getour Ô¨Ånal MAP measure:MAP=1|Q|Xq2QAP(q)(11.16)The MAP for the single query (hence = AP) in Fig.11.3is 0.6.Information Retrieval and RAGEvaluation of IRInformation Retrieval and RAGDense RetrievalProblem with classic IRThe vocabulary mismatch problem‚Ä¢tf-idf or BM25 cosine similarities only work if there is exact word overlap between query and doc!‚Ä¢But query-writer can't know the exact words the doc might include!Dense retrievalInstead of representing query and documents with count vectorsRepresent both with embeddings!Hypothetical version of dense retrieval: static embeddingsReplace tf-idf vectors with, e.g., word2vec‚Ä¢Query: the mean of the embeddings of each query word‚Ä¢Doc: the mean of the doc word embeddings‚Ä¢Now just compute query-doc cosine as normal.We don't do this because contextual embeddings work much better!Dense retrieval #1: Single encoder11.2‚Ä¢INFORMATIONRETRIEV AL WITHDENSEVECTORS1111.2 Information Retrieval with Dense VectorsThe classic tf-idf or BM25 algorithms for IR have long been known to have a con-ceptual Ô¨Çaw: they work only if there is exact overlap of words between the queryand document. In other words, the user posing a query (or asking a question) needsto guess exactly what words the writer of the answer might have used, an issue calledthevocabulary mismatch problem(Furnas et al.,1987).The solution to this problem is to use an approach that can handle synonymy:instead of (sparse) word-count vectors, using (dense) embeddings. This idea wasÔ¨Årst proposed for retrieval in the last century under the name of Latent SemanticIndexing approach (Deerwester et al.,1990), but is implemented in modern timesvia encoders like BERT.The most powerful approach is to present both the query and the document to asingle encoder, allowing the transformer self-attention to see all the tokens of boththe query and the document, and thus building a representation that is sensitive tothe meanings of both query and document. Then a linear layer can be put on top ofthe [CLS] token to predict a similarity score for the query/document tuple:z=BERT(q;[SEP];d)[CLS]score(q,d)=softmax(U(z))(11.17)This architecture is shown in Fig.11.7a. Usually the retrieval step is not done onan entire document. Instead documents are broken up into smaller passages, suchas non-overlapping Ô¨Åxed-length chunks of say 100 tokens, and the retriever encodesand retrieves these passages rather than entire documents. The query and documenthave to be made to Ô¨Åt in the BERT 512-token window, for example by truncatingthe query to 64 tokens and truncating the document if necessary so that it, the query,[CLS], and [SEP] Ô¨Åt in 512 tokens. The BERT system together with the linear layerUcan then be Ô¨Åne-tuned for the relevance task by gathering a tuning dataset ofrelevant and non-relevant passages.The problem with the full BERT architecture in Fig.11.7a is the expense incomputation and time. With this architecture, every time we get a query, we have topass every single document in our entire collection through a BERT encoder jointlywith the new query! This enormous use of resources is impractical for real cases.At the other end of the computational spectrum is a much more efÔ¨Åcient archi-tecture, thebi-encoder. In this architecture we can encode the documents in thecollection only one time by using two separate encoder models, one to encode thequery and one to encode the document. We encode each document, and store allthe encoded document vectors in advance. When a query comes in, we encode justthis query and then use the dot product between the query vector and the precom-puted document vectors as the score for each candidate document (Fig.11.7b). Forexample, if we used BERT, we would have two encoders BERTQand BERTDandwe could represent the query and document as the[CLS]token of the respectiveencoders (Karpukhin et al.,2020):zq=BERTQ(q)[CLS]zd=BERTD(d)[CLS]score(q,d)=zq¬∑zd(11.18)The bi-encoder is much cheaper than a full query/document encoder, but is alsoless accurate, since its relevance decision can‚Äôt take full advantage of all the possi-12CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS
QueryDocument‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¢s(q,d)
(a) (b)Figure 11.7Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-resent self-attention: (a) Use a single encoder to jointly encode query and document and Ô¨Ånetune to produce arelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for thequery and document as the score. This is less compute-expensive, but not as accurate.ble meaning interactions between all the tokens in the query and the tokens in thedocument.There are numerous approaches that lie in between the full encoder and the bi-encoder. One intermediate alternative is to use cheaper methods (like BM25) as theÔ¨Årst pass relevance ranking for each document, take the top N ranked documents,and use expensive methods like the full BERT scoring to rerank only the top Ndocuments rather than the whole set.Another intermediate approach is theColBERTapproach ofKhattab and Za-ColBERTharia(2020) andKhattab et al.(2021), shown in Fig.11.8. This method separatelyencodes the query and document, but rather than encoding the entire query or doc-ument into one vector, it separately encodes each of them into contextual represen-tations for each token. These BERT representations of each document word can bepre-stored for efÔ¨Åciency. The relevance score between a queryqand a documentdisa sum of maximum similarity (MaxSim) operators between tokens inqand tokensind. Essentially, for each token inq, ColBERT Ô¨Ånds the most contextually simi-lar token ind, and then sums up these similarities. A relevant document will havetokens that are contextually very similar to the query.More formally, a questionqis tokenized as[q1,...,qn], prepended with a[CLS]and a special[Q]token, truncated to N=32 tokens (or padded with[MASK]tokens ifit is shorter), and passed through BERT to get output vectorsq=[q1,...,qN]. Thepassagedwith tokens[d1,...,dm], is processed similarly, including a[CLS]andspecial[D]token. A linear layer is applied on top ofdandqto control the outputdimension, so as to keep the vectors small for storage efÔ¨Åciency, and vectors arerescaled to unit length, producing the Ô¨Ånal vector sequencesEq(lengthN) andEd(lengthm). The ColBERT scoring mechanism is:score(q,d)=NXi=1mmaxj=1Eqi¬∑Edj(11.19)While the interaction mechanism has no tunable parameters, the ColBERT ar-Dense retrieval #1: Single encoderSystem is run on passages (say 100 tokens) instead of whole documentsTraining:‚Ä¢BERT and linear layer U can then fine-tuned for relevance‚Ä¢Creating a tuning dataset of relevant and non-relevant passages. 12CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS
QueryDocument‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¢s(q,d)
(a) (b)Figure 11.7Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-resent self-attention: (a) Use a single encoder to jointly encode query and document and Ô¨Ånetune to produce arelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for thequery and document as the score. This is less compute-expensive, but not as accurate.ble meaning interactions between all the tokens in the query and the tokens in thedocument.There are numerous approaches that lie in between the full encoder and the bi-encoder. One intermediate alternative is to use cheaper methods (like BM25) as theÔ¨Årst pass relevance ranking for each document, take the top N ranked documents,and use expensive methods like the full BERT scoring to rerank only the top Ndocuments rather than the whole set.Another intermediate approach is theColBERTapproach ofKhattab and Za-ColBERTharia(2020) andKhattab et al.(2021), shown in Fig.11.8. This method separatelyencodes the query and document, but rather than encoding the entire query or doc-ument into one vector, it separately encodes each of them into contextual represen-tations for each token. These BERT representations of each document word can bepre-stored for efÔ¨Åciency. The relevance score between a queryqand a documentdisa sum of maximum similarity (MaxSim) operators between tokens inqand tokensind. Essentially, for each token inq, ColBERT Ô¨Ånds the most contextually simi-lar token ind, and then sums up these similarities. A relevant document will havetokens that are contextually very similar to the query.More formally, a questionqis tokenized as[q1,...,qn], prepended with a[CLS]and a special[Q]token, truncated to N=32 tokens (or padded with[MASK]tokens ifit is shorter), and passed through BERT to get output vectorsq=[q1,...,qN]. Thepassagedwith tokens[d1,...,dm], is processed similarly, including a[CLS]andspecial[D]token. A linear layer is applied on top ofdandqto control the outputdimension, so as to keep the vectors small for storage efÔ¨Åciency, and vectors arerescaled to unit length, producing the Ô¨Ånal vector sequencesEq(lengthN) andEd(lengthm). The ColBERT scoring mechanism is:score(q,d)=NXi=1mmaxj=1Eqi¬∑Edj(11.19)While the interaction mechanism has no tunable parameters, the ColBERT ar-Dense retrieval #2 (biencoder)12CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS
QueryDocument‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¢s(q,d)
(a) (b)Figure 11.7Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-resent self-attention: (a) Use a single encoder to jointly encode query and document and Ô¨Ånetune to produce arelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for thequery and document as the score. This is less compute-expensive, but not as accurate.ble meaning interactions between all the tokens in the query and the tokens in thedocument.There are numerous approaches that lie in between the full encoder and the bi-encoder. One intermediate alternative is to use cheaper methods (like BM25) as theÔ¨Årst pass relevance ranking for each document, take the top N ranked documents,and use expensive methods like the full BERT scoring to rerank only the top Ndocuments rather than the whole set.Another intermediate approach is theColBERTapproach ofKhattab and Za-ColBERTharia(2020) andKhattab et al.(2021), shown in Fig.11.8. This method separatelyencodes the query and document, but rather than encoding the entire query or doc-ument into one vector, it separately encodes each of them into contextual represen-tations for each token. These BERT representations of each document word can bepre-stored for efÔ¨Åciency. The relevance score between a queryqand a documentdisa sum of maximum similarity (MaxSim) operators between tokens inqand tokensind. Essentially, for each token inq, ColBERT Ô¨Ånds the most contextually simi-lar token ind, and then sums up these similarities. A relevant document will havetokens that are contextually very similar to the query.More formally, a questionqis tokenized as[q1,...,qn], prepended with a[CLS]and a special[Q]token, truncated to N=32 tokens (or padded with[MASK]tokens ifit is shorter), and passed through BERT to get output vectorsq=[q1,...,qN]. Thepassagedwith tokens[d1,...,dm], is processed similarly, including a[CLS]andspecial[D]token. A linear layer is applied on top ofdandqto control the outputdimension, so as to keep the vectors small for storage efÔ¨Åciency, and vectors arerescaled to unit length, producing the Ô¨Ånal vector sequencesEq(lengthN) andEd(lengthm). The ColBERT scoring mechanism is:score(q,d)=NXi=1mmaxj=1Eqi¬∑Edj(11.19)While the interaction mechanism has no tunable parameters, the ColBERT ar-11.2‚Ä¢INFORMATIONRETRIEV AL WITHDENSEVECTORS1111.2 Information Retrieval with Dense VectorsThe classic tf-idf or BM25 algorithms for IR have long been known to have a con-ceptual Ô¨Çaw: they work only if there is exact overlap of words between the queryand document. In other words, the user posing a query (or asking a question) needsto guess exactly what words the writer of the answer might have used, an issue calledthevocabulary mismatch problem(Furnas et al.,1987).The solution to this problem is to use an approach that can handle synonymy:instead of (sparse) word-count vectors, using (dense) embeddings. This idea wasÔ¨Årst proposed for retrieval in the last century under the name of Latent SemanticIndexing approach (Deerwester et al.,1990), but is implemented in modern timesvia encoders like BERT.The most powerful approach is to present both the query and the document to asingle encoder, allowing the transformer self-attention to see all the tokens of boththe query and the document, and thus building a representation that is sensitive tothe meanings of both query and document. Then a linear layer can be put on top ofthe [CLS] token to predict a similarity score for the query/document tuple:z=BERT(q;[SEP];d)[CLS]score(q,d)=softmax(U(z))(11.17)This architecture is shown in Fig.11.7a. Usually the retrieval step is not done onan entire document. Instead documents are broken up into smaller passages, suchas non-overlapping Ô¨Åxed-length chunks of say 100 tokens, and the retriever encodesand retrieves these passages rather than entire documents. The query and documenthave to be made to Ô¨Åt in the BERT 512-token window, for example by truncatingthe query to 64 tokens and truncating the document if necessary so that it, the query,[CLS], and [SEP] Ô¨Åt in 512 tokens. The BERT system together with the linear layerUcan then be Ô¨Åne-tuned for the relevance task by gathering a tuning dataset ofrelevant and non-relevant passages.The problem with the full BERT architecture in Fig.11.7a is the expense incomputation and time. With this architecture, every time we get a query, we have topass every single document in our entire collection through a BERT encoder jointlywith the new query! This enormous use of resources is impractical for real cases.At the other end of the computational spectrum is a much more efÔ¨Åcient archi-tecture, thebi-encoder. In this architecture we can encode the documents in thecollection only one time by using two separate encoder models, one to encode thequery and one to encode the document. We encode each document, and store allthe encoded document vectors in advance. When a query comes in, we encode justthis query and then use the dot product between the query vector and the precom-puted document vectors as the score for each candidate document (Fig.11.7b). Forexample, if we used BERT, we would have two encoders BERTQand BERTDandwe could represent the query and document as the[CLS]token of the respectiveencoders (Karpukhin et al.,2020):zq=BERTQ(q)[CLS]zd=BERTD(d)[CLS]score(q,d)=zq¬∑zd(11.18)The bi-encoder is much cheaper than a full query/document encoder, but is alsoless accurate, since its relevance decision can‚Äôt take full advantage of all the possi-Dense retrieval #2 (biencoder)Encode doc vectors in advance. Encode query when it arrives‚Ä¢Score is dot product between query vector and precomputed doc vector‚Ä¢Cheaper but less accurate12CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS
QueryDocument‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¢s(q,d)
(a) (b)Figure 11.7Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-resent self-attention: (a) Use a single encoder to jointly encode query and document and Ô¨Ånetune to produce arelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for thequery and document as the score. This is less compute-expensive, but not as accurate.ble meaning interactions between all the tokens in the query and the tokens in thedocument.There are numerous approaches that lie in between the full encoder and the bi-encoder. One intermediate alternative is to use cheaper methods (like BM25) as theÔ¨Årst pass relevance ranking for each document, take the top N ranked documents,and use expensive methods like the full BERT scoring to rerank only the top Ndocuments rather than the whole set.Another intermediate approach is theColBERTapproach ofKhattab and Za-ColBERTharia(2020) andKhattab et al.(2021), shown in Fig.11.8. This method separatelyencodes the query and document, but rather than encoding the entire query or doc-ument into one vector, it separately encodes each of them into contextual represen-tations for each token. These BERT representations of each document word can bepre-stored for efÔ¨Åciency. The relevance score between a queryqand a documentdisa sum of maximum similarity (MaxSim) operators between tokens inqand tokensind. Essentially, for each token inq, ColBERT Ô¨Ånds the most contextually simi-lar token ind, and then sums up these similarities. A relevant document will havetokens that are contextually very similar to the query.More formally, a questionqis tokenized as[q1,...,qn], prepended with a[CLS]and a special[Q]token, truncated to N=32 tokens (or padded with[MASK]tokens ifit is shorter), and passed through BERT to get output vectorsq=[q1,...,qN]. Thepassagedwith tokens[d1,...,dm], is processed similarly, including a[CLS]andspecial[D]token. A linear layer is applied on top ofdandqto control the outputdimension, so as to keep the vectors small for storage efÔ¨Åciency, and vectors arerescaled to unit length, producing the Ô¨Ånal vector sequencesEq(lengthN) andEd(lengthm). The ColBERT scoring mechanism is:score(q,d)=NXi=1mmaxj=1Eqi¬∑Edj(11.19)While the interaction mechanism has no tunable parameters, the ColBERT ar-In-between dense retrieval methodsUse cheap methods (like BM25) as first pass relevance ranking for each document, Then just rerank the top N ranked docs, ‚Ä¢Using expensive methods like the full BERT scoringColBERTPrecompute document but store each word vectorThen do maxsim between document and query wordsKhattab and Zaharia (2020), Khattab et al (2021)ColBERT12CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELS
QueryDocument‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¢s(q,d)
(a) (b)Figure 11.7Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-resent self-attention: (a) Use a single encoder to jointly encode query and document and Ô¨Ånetune to produce arelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for thequery and document as the score. This is less compute-expensive, but not as accurate.ble meaning interactions between all the tokens in the query and the tokens in thedocument.There are numerous approaches that lie in between the full encoder and the bi-encoder. One intermediate alternative is to use cheaper methods (like BM25) as theÔ¨Årst pass relevance ranking for each document, take the top N ranked documents,and use expensive methods like the full BERT scoring to rerank only the top Ndocuments rather than the whole set.Another intermediate approach is theColBERTapproach ofKhattab and Za-ColBERTharia(2020) andKhattab et al.(2021), shown in Fig.11.8. This method separatelyencodes the query and document, but rather than encoding the entire query or doc-ument into one vector, it separately encodes each of them into contextual represen-tations for each token. These BERT representations of each document word can bepre-stored for efÔ¨Åciency. The relevance score between a queryqand a documentdisa sum of maximum similarity (MaxSim) operators between tokens inqand tokensind. Essentially, for each token inq, ColBERT Ô¨Ånds the most contextually simi-lar token ind, and then sums up these similarities. A relevant document will havetokens that are contextually very similar to the query.More formally, a questionqis tokenized as[q1,...,qn], prepended with a[CLS]and a special[Q]token, truncated to N=32 tokens (or padded with[MASK]tokens ifit is shorter), and passed through BERT to get output vectorsq=[q1,...,qN]. Thepassagedwith tokens[d1,...,dm], is processed similarly, including a[CLS]andspecial[D]token. A linear layer is applied on top ofdandqto control the outputdimension, so as to keep the vectors small for storage efÔ¨Åciency, and vectors arerescaled to unit length, producing the Ô¨Ånal vector sequencesEq(lengthN) andEd(lengthm). The ColBERT scoring mechanism is:score(q,d)=NXi=1mmaxj=1Eqi¬∑Edj(11.19)While the interaction mechanism has no tunable parameters, the ColBERT ar-11.2‚Ä¢INFORMATIONRETRIEV AL WITHDENSEVECTORS13
QueryDocument‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶s(q,d)MaxSimMaxSimMaxSim‚àë
normnormnormnormnormnorm
Figure 11.8A sketch of the ColBERT algorithm at inference time. The query and docu-ment are Ô¨Årst passed through separate BERT encoders. Similarity between query and doc-ument is computed by summing a soft alignment between the contextual representations oftokens in the query and the document. Training is end-to-end. (Various details aren‚Äôt de-picted; for example the query is prepended by a[CLS]and[Q:]tokens, and the documentby[CLS]and[D:]tokens). Figure adapted fromKhattab and Zaharia(2020).chitecture still needs to be trained end-to-end to Ô¨Åne-tune the BERT encoders andtrain the linear layers (and the special[Q]and[D]embeddings) from scratch. It istrained on tripleshq,d+,d iof queryq, positive documentd+and negative docu-mentd to produce a score for each document using Eq.11.19, optimizing modelparameters using a cross-entropy loss.All the supervised algorithms (like ColBERT or the full-interaction version ofthe BERT algorithm applied for reranking) need training data in the form of queriestogether with relevant and irrelevant passages or documents (positive and negativeexamples). There are various semi-supervised ways to get labels; some datasets(like MS MARCO Ranking, Section11.4) contain gold positive examples. Negativeexamples can be sampled randomly from the top-1000 results from some existingIR system. If datasets don‚Äôt have labeled positive examples, iterative methods likerelevance-guided supervisioncan be used (Khattab et al.,2021) which rely on thefact that many datasets contain short answer strings. In this method, an existing IRsystem is used to harvest examples that do contain short answer strings (the top feware taken as positives) or don‚Äôt contain short answer strings (the top few are taken asnegatives), these are used to train a new retriever, and then the process is iterated.EfÔ¨Åciency is an important issue, since every possible document must be rankedfor its similarity to the query. For sparse word-count vectors, the inverted indexallows this very efÔ¨Åciently. For dense vector algorithms Ô¨Ånding the set of densedocument vectors that have the highest dot product with a dense query vector isan instance of the problem ofnearest neighbor search. Modern systems there-fore make use of approximate nearest neighbor vector search algorithms likeFaissFaiss(Johnson et al.,2017).Training for dense retrievalColBERT and other models need to be trained‚Ä¢To fine-tune the BERT encoders and train the linear layers (and the special [Q] and [D] embeddings) ‚Ä¢On datasets of  triples ‚ü®q,d+,d‚àí‚ü©‚Ä¢Some datasets like MS MARCO Ranking have positive examplesEfficiency in dense retrievalWe must rank every document for its similarity to the query!Efficiency for sparse word-count vectors: inverted indexEfficiency for dense retrieval:‚Ä¢nearest neighbor search: finding the set of dense document vectors that have the highest dot product with a dense query vector. ‚Ä¢Approximate nearest neighbor algorithm Faiss (Johnson et al., 2017). ‚Ä¢Approximates the doc vector by a smaller quantized vectorInformation Retrieval and RAGDense RetrievalInformation Retrieval and RAGRetrieval-Augmented GenerationIR plays a central role in modern LLMsHow to answer factual questions like‚Ä¢Where is the Louvre Museum located?‚Ä¢How to get a script l in latex?‚Ä¢Where does the energy in a nuclear explosion come from?Just prompt an LLM!
LLMs seem to store facts in the connections in their feedforward layers!‚àô
The Louvre, or the Louvre Museum (French: Mus√©e du Louvre[myze dy luv Å]), is a national art museum in Paris, France, and‚Ä¶
10/31/25, 9:47 AMWhere is the Louvre Museum located? - Google Search
https://www.google.com/search?q=Where+is+the+Louvre+Museum+located%3F&rlz=1C5GCCM_en&oq=Where+is+the+Louvre+Museum+located%3F&gs_lcrp=‚Ä¶1/7But there are issues!LLMs HallucinateHallucination: a response that is not faithful to the facts of the world. In the legal domain LLMs were shown to hallucinate up to 88% of the time!Dahl et al. (2024)Can't use Proprietary DataPeople need to ask questions about: ‚Ä¢personal email. ‚Ä¢healthcare applications to medical records.‚Ä¢internal corporate documents ‚Ä¢legal documents discoveryCan't Handle Dynamic DataLLMs can't answer questions about rapidly changing informationThings that happened last weekIn general, data shifts over timeSolution: RAGRetrieval-Augmented Generation1.Use IR to retrieve documents from some collection2.Then use LLM to generate an answer conditioned on the documentsRetrieval Augmented Generation (RAG)
When wasthe premiere ofThe Magic Flute?RelevantDocs1791, according to this pageRetrieverIndexed Docs
LLMGeneratorCorpus ofDocumentsUser prompt:Prompt formulationKnowledgeCitationBasic RAGGiven a document collection D and a user query q‚Ä¢Call a retriever to return top k passages ‚Ä¢Create a prompt that includes q and the passages‚Ä¢Call an LLM with the prompt11.4‚Ä¢RETRIEV AL-AUGMENTEDGENERATION(RAG)17collection2.Create a prompt from the user prompt, the retrieved passages, and some addi-tional text.3.Call an LLM with the prompt.The resulting prompts might look something like:Schematic of a RAG Promptretrieved passage 1retrieved passage 2...retrieved passage kBased on these texts, answer this question: What yearwas the premiere ofThe Magic Flute?The task for the language model is then to generate text according to this proba-bility model:p(x1,...,xn)=nYi=1p(xi|R(q) ; Answer the following question... ;q;x<i)There are many augmentations of this basic RAG paradigm. One addition isthe use ofagent-based RAG. In the RAG paradigm described so far, a search isalways run and then retrieved passages are combined with the user‚Äôs question in aprompt. But in actual applications, we may not want to run retrieval for every userturn. Or we may want to retrieve from different collections for different user needs(sometimes the web, other times a private collection). Inagent-based RAG, thesystem decides when to call a retrieval agent and for which collection.Another research area has to do with the relationship between the retriever andthe generator. For example there may be noise in the retrieved passages; some ofthem may be irrelevant or wrong, or in an unhelpful order. How can we encouragethe LLM to focus on the good passages? Some RAG architectures add a rerankerthat reranks or reorders passages after they are retrieved. Or some complex questionsmay require multi-hop architectures, in which a query is used to retrieve documents,which are then appended to the original query for a second stage of retrieval.Another class of solutions is to train the LLM for RAG. The basic version ofRAG describe above involves no training; we take an off-the-shelf LLM, and giveit the passages and a prompt and hope that it will correctly Ô¨Ågure out which pas-sages are useful or relevant in generating the answer. One learning variant involvesinstruction-tuning an LLM, by Ô¨Årst creating a dataset of questions annotated withretrieved passages and correct answers, and then instruction-tuning the LLM to cor-rectly answer the questions from the passages. An alternative method is to do this viatest-time compute, prompting the LLM to answer the question and simultaneously togenerate reÔ¨Çections on which passages were useful. The process of generating thesereÔ¨Çections may lead the LLM to improve at identifying good passages. The result-ing reÔ¨Çection text can also be used for in-context learning, for example by using thetext as part of a prompt for further questions.11.4‚Ä¢RETRIEV AL-AUGMENTEDGENERATION(RAG)17collection2.Create a prompt from the user prompt, the retrieved passages, and some addi-tional text.3.Call an LLM with the prompt.The resulting prompts might look something like:Schematic of a RAG Promptretrieved passage 1retrieved passage 2...retrieved passage kBased on these texts, answer this question: What yearwas the premiere ofThe Magic Flute?The task for the language model is then to generate text according to this proba-bility model:p(x1,...,xn)=nYi=1p(xi|R(q) ; Answer the following question... ;q;x<i)There are many augmentations of this basic RAG paradigm. One addition isthe use ofagent-based RAG. In the RAG paradigm described so far, a search isalways run and then retrieved passages are combined with the user‚Äôs question in aprompt. But in actual applications, we may not want to run retrieval for every userturn. Or we may want to retrieve from different collections for different user needs(sometimes the web, other times a private collection). Inagent-based RAG, thesystem decides when to call a retrieval agent and for which collection.Another research area has to do with the relationship between the retriever andthe generator. For example there may be noise in the retrieved passages; some ofthem may be irrelevant or wrong, or in an unhelpful order. How can we encouragethe LLM to focus on the good passages? Some RAG architectures add a rerankerthat reranks or reorders passages after they are retrieved. Or some complex questionsmay require multi-hop architectures, in which a query is used to retrieve documents,which are then appended to the original query for a second stage of retrieval.Another class of solutions is to train the LLM for RAG. The basic version ofRAG describe above involves no training; we take an off-the-shelf LLM, and giveit the passages and a prompt and hope that it will correctly Ô¨Ågure out which pas-sages are useful or relevant in generating the answer. One learning variant involvesinstruction-tuning an LLM, by Ô¨Årst creating a dataset of questions annotated withretrieved passages and correct answers, and then instruction-tuning the LLM to cor-rectly answer the questions from the passages. An alternative method is to do this viatest-time compute, prompting the LLM to answer the question and simultaneously togenerate reÔ¨Çections on which passages were useful. The process of generating thesereÔ¨Çections may lead the LLM to improve at identifying good passages. The result-ing reÔ¨Çection text can also be used for in-context learning, for example by using thetext as part of a prompt for further questions.Extensions: Agent-based RAGInstead of running RAG automatically on every user turnHave a retrieval agents System decides when to call it and for which document collectionExtensions: TrainingInstruction-tune an LLM on a dataset of questions with retrieved passages and correct answersTest-time compute: prompt an LLM to answer the question and simultaneously to generate reflections on which passages were usefulExtensions: Knowledge Citations18CHAPTER11‚Ä¢RETRIEV AL-BASEDMODELSIn addition to training the LLM, we could train the IR engine. After all, the IRengine itself has not been optimized for the RAG scenario. It might not have beentrained, or if it was, it was likely trained for simple IR or factoid question-answeringtasks, not for the RAG scenario where the retrieved passages are speciÔ¨Åcally to beused by another LLM for generating texts. We can address this mismatch for train-able IR algorithms by doing end-to-end training of the entire architecture on someset of questions and answers, training the parameters of the IR model as well as theLLM.Finally, it is generally useful for LLMs to give the user evidence for any factualstatement. This can be in the form ofknowledge citations, such as URLs of aknowledgecitationstrusted source or citation references to particular literature. For example a questionanswering system might generate numbered pointers to URLs as follows:Q: Which Ô¨Ålms have Gong Li as a member of their cast?A: The Story of Qiu Ju [1], Farewell My Concubine [2], The MonkeyKing 2 [3], Mulan [3], Saturday Fiction [3]...The simplest way for generating knowledge citations is to specify it as part ofthe prompt. For exampleGao et al.(2023) employ a prompt with text like:‚Äò‚ÄòWrite an answer for the given question using only theprovided search results (some of which might be irrelevant)and cite them properly... Always cite for any factual claim".11.5 DatasetsThere are scores of datasets that contain information needs in the form of questions,annotated with the answer. These can be used both for instruction tuning and forevaluation of the question answering abilities of language models.We can distinguish the datasets along many dimensions, summarized nicely inRogers et al.(2023). One is the original purpose of the questions in the data, whetherthey were naturalinformation-seekingquestions, or whether they were questionsdesigned forprobing: evaluating or testing systems or humans.On the natural side there are datasets likeNatural Questions(KwiatkowskiNaturalQuestionset al.,2019), a set of anonymized English queries to the Google search engine andtheir answers. The answers are created by annotators based on Wikipedia infor-mation, and include a paragraph-length long answer and a short span answer. Forexample the question‚ÄúWhen are hops added to the brewing process?‚Äùhas the shortanswerthe boiling processand a long answer which is an entire paragraph from theWikipedia page onBrewing.A similar natural question set is theMS MARCO(Microsoft Machine ReadingMS MARCOComprehension) collection of datasets, including 1 million real anonymized Englishquestions from Microsoft Bing query logs together with a human generated answerand 9 million passages (Bajaj et al.,2016), that can be used both to test retrievalranking and question answering.Although many datasets focus on English, natural information-seeking ques-tion datasets exist in other languages. The DuReader dataset is a Chinese QA re-source based on search engine queries and community QA (He et al.,2018).TyDiQAdataset contains 204K question-answer pairs from 11 typologically diverse lan-TyDi QAguages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al.,2020).In the TYDIQA task, a system is given a question and the passages from a Wiki-‚Äò‚ÄòWrite an answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly...  Always cite for any factual claim".Information Retrieval and RAGRetrieval-Augmented GenerationInformation Retrieval and RAGQuestion Answering datasets and evalsTwo kinds of question answering datasetsNatural information-seeking questions‚Ä¢ Someone actually wanted to know the answer to thisProbing (testing) questions‚Ä¢Exam-type questions for LLM evaluationNatural Questions (Kwiatkowski et al., 2019), anonymized English queries to the Google search engine and short and long answers  (hand-created from Wikipedia)‚ÄúWhen are hops added to the brewing process?‚Äù short answer: the boiling process long answer: paragraph from the Wikipedia page on BrewingMS MARCO (Microsoft Machine Reading Comprehension) collection of datasets, 1 million real anonymized English questions from Microsoft Bing query logshuman generated answer 9 million passages (Bajaj et al., 2016)Probing dataset: MMLU15908 knowledge and reasoning questions in 57 areas including medicine, mathematics, computer science, law, etc.. Sourced from exams for humans like GRE, AP 11.5‚Ä¢DATASETS19pedia article and must (a) select the passage containing the answer (or NULLif nopassage contains the answer), and (b) mark the minimal answer span (or NULL).On the probing side are datasets likeMMLU(Massive Multitask Language Un-MMLUderstanding), a commonly-used dataset of 15908 knowledge and reasoning ques-tions in 57 areas including medicine, mathematics, computer science, law, and oth-ers. MMLU questions are sourced from various exams for humans, such as the USGraduate Record Exam, Medical Licensing Examination, and Advanced Placementexams. So the questions don‚Äôt represent people‚Äôs information needs, but rather aredesigned to test human knowledge for academic or licensing purposes. Fig.11.14shows some examples, with the correct answers in bold.MMLU examplesCollege Computer ScienceAny set of Boolean operators that is sufÔ¨Åcient to represent all Boolean ex-pressions is said to be complete. Which of the following is NOT complete?(A) AND, NOT(B) NOT, OR(C) AND, OR(D) NANDCollege PhysicsThe primary source of the Sun‚Äôs energy is a series of thermonuclearreactions in which the energy produced is c2times the mass differencebetween(A) two hydrogen atoms and one helium atom(B)four hydrogen atoms and one helium atom(C) six hydrogen atoms and two helium atoms(D) three helium atoms and one carbon atomInternational LawWhich of the following is a treaty-based human rights mechanism?(A)The UN Human Rights Committee(B) The UN Human Rights Council(C) The UN Universal Periodic Review(D) The UN special mandatesPrehistoryUnlike most other early civilizations, Minoan culture shows little evidenceof(A) trade.(B) warfare.(C) the development of a common religion.(D)conspicuous consumption by elites.Figure 11.14Example problems from MMLUSome of the question datasets described above augment each question with pas-sage(s) from which the answer can be extracted. These datasets were mainly createdfor an earlier QA task calledreading comprehensionin which a model is givenreadingcomprehensiona question and a document and is required to extract the answer from the givenInformation Retrieval and RAGQuestion Answering datasets and evals